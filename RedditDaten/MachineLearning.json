{"submission": [{"author": "MTGTraner", "id": "8kbmyn", "title": "[D] If you had to show one paper to someone to show that machine learning is beautiful, what would you choose? (assuming they're equipped to understand it)", "url": "https://www.reddit.com/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/", "viewcount": null, "content": "", "upvote": 1023, "downvote": 0, "upratio": 0.98, "subreddit": "r/MachineLearning", "comments": [{"author": "svaisakh", "id": "dz6flte", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6flte/", "content": "The [InfoGAN](https://arxiv.org/abs/1606.03657) was the first paper which really opened my eyes to the potential of unsupervised learning.\n\nWith nothing but raw data, the model learned abstract concepts like 'rotation', 'width' and 'stroke-thickness'.", "score": 210, "likes": 0, "upvote": 210, "downvote": 0, "comments": [{"author": "CraftyColossus", "id": "dz6hgti", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hgti/", "content": "Yup, that is my current go to paper to describe how cool unsupervised learning is.", "score": 28, "likes": 0, "upvote": 28, "downvote": 0, "comments": 0}, {"author": "MyNameIsJonny_", "id": "dz8a9mc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz8a9mc/", "content": "That was fascinating thank you. It was also very easy to follow, even as a lowly stats undergrad.", "score": 14, "likes": 0, "upvote": 14, "downvote": 0, "comments": [{"author": "aunick2017", "id": "e18n8vv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e18n8vv/", "content": "What are the background topics needed to follow this? I have a stats background too, but I found it hard to follow", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "sciencethat", "id": "e0lmcm4", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0lmcm4/", "content": "Just came across this. I've been wanting to start reading research papers for a while now. I finally started reading through InfoGAN but found I'm lacking a lot of other things to I should probably know in advance. Could you recommend a starter ML paper for me to just get the hang of how research papers are actually written?", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "None", "id": "dz6vib9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6vib9/", "content": "[deleted]", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "zhangqianhui", "id": "e11r2nn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e11r2nn/", "content": "The distangled representation learning has been researched for a long time.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "BotPaperScissors", "id": "e3ol4ol", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3ol4ol/", "content": "Paper! \u270b We drew", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "jiamengial", "id": "dz6fxa7", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6fxa7/", "content": "[A Unifying Review of Linear Gaussian Models](http://mlg.eng.cam.ac.uk/zoubin/papers/lds.pdf)", "score": 76, "likes": 0, "upvote": 76, "downvote": 0, "comments": [{"author": "ccmlacc", "id": "dz6qicz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6qicz/", "content": "This paper is like poetry. My favorite by far!", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}, {"author": "jsnoek", "id": "dzn2fhd", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzn2fhd/", "content": "A classic!", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "mr_tsjolder", "id": "e11rt9w", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e11rt9w/", "content": "multidimensional pancakes...", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "Stone_d_", "id": "dz770rc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz770rc/", "content": "What a fantastic Reddit post. Love it", "score": 62, "likes": 0, "upvote": 62, "downvote": 0, "comments": 0}, {"author": "ReginaldIII", "id": "dz6ha44", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6ha44/", "content": "I have a fondness for [[Gatys et. al. 2015]](https://arxiv.org/abs/1508.06576)'s seminal work on neural artistic style transfer. There is a simplicity and elegance in the use of Gram matrices that made me want to understand how on earth they could convey stylistic similarity so well. ", "score": 97, "likes": 0, "upvote": 97, "downvote": 0, "comments": [{"author": "shortscience_dot_org", "id": "dz6hamg", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hamg/", "content": "I am a bot! You linked to a paper that has a summary on ShortScience.org!\n\n**A Neural Algorithm of Artistic Style** \n\n*Summary by Alexander Jung*\n\n* The paper describes a method to separate content and style from each other in an image.\n\n  * The style can then be transfered to a new image.\n\n  * Examples:\n\n    * Let a photograph look like a painting of van Gogh.\n\n    * Improve a dark beach photo by taking the style from a sunny beach photo.\n\n\n\n### How\n\n  * They use the pretrained 19-layer VGG net as their base network.\n\n  * They assume that two images are provided: One with the *content*, one with the desired *style*.\n\n  * They feed the content i... [[view more]](http://www.shortscience.org/paper?bibtexKey=journals/corr/1508.06576)", "score": 152, "likes": 0, "upvote": 152, "downvote": 0, "comments": [{"author": "ReginaldIII", "id": "dz6hym8", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hym8/", "content": "Good bot. Actually very good bot. ", "score": 109, "likes": 0, "upvote": 109, "downvote": 0, "comments": [{"author": "PrancingPeach", "id": "dz7npge", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7npge/", "content": "I briefly overlooked the top few lines and thought the bot actually generated the summary algorithmically, and I was like, \"damn, I want to see the paper on the bot now...\"", "score": 20, "likes": 0, "upvote": 20, "downvote": 0, "comments": 0}, {"author": "GoodBot_BadBot", "id": "dz6hyqa", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hyqa/", "content": "Thank you, ReginaldIII, for voting on shortscience\\_dot\\_org.  \n\nThis bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/).  \n\n ***  \n\n^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!", "score": 9, "likes": 0, "upvote": 9, "downvote": 0, "comments": [{"author": "poopypoopersonIII", "id": "dz6uils", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6uils/", "content": "Good bot", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "therealkainoa", "id": "dz75kd7", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz75kd7/", "content": "Bad person ", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": [{"author": "poopypoopersonIII", "id": "dz76wkt", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz76wkt/", "content": "No u", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "agree-with-you", "id": "dz76wpv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz76wpv/", "content": "No you both", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "scionaura", "id": "dz7kipw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7kipw/", "content": "Me too thanks", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}]}]}]}]}]}, {"author": "daymanAAaah", "id": "dz6xw6u", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6xw6u/", "content": "I thought I was reading a person\u2019s comment until I saw yours. Damn that\u2019s good.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "ReginaldIII", "id": "dz700gs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz700gs/", "content": "You are reading a a person's comment. It's a summary aggregate tool which has shown us a user written paper summary that can be voted and commented on just like a reddit thread. \n\nI much prefer this style of summary bot to fully automated ones that generate the text as they tend skew the sentiment of the text and misplace emphasis.", "score": 9, "likes": 0, "upvote": 9, "downvote": 0, "comments": 0}]}, {"author": "progfu", "id": "dzxw32h", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzxw32h/", "content": "I wish all papers had 1-2 sentence summary that actually describes wtf is going on. I guess it should be the abstract, but so often I'm looking at something and just trying to figure out wtf.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "the1fundamental", "id": "e09ck5n", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e09ck5n/", "content": "good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "bhargav-patel", "id": "e0t8bu9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0t8bu9/", "content": "good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "bradfordmaster", "id": "dz72cex", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz72cex/", "content": "Good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "SmasimGirl", "id": "dz7dbn1", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7dbn1/", "content": "Good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "fucksfired", "id": "dz80f1i", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz80f1i/", "content": "Good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "gwern", "id": "dz6pjzs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6pjzs/", "content": "I was thinking of style transfer too. It is a completely unexpected result from an approach which seems like it shouldn't do anything, and it produces results a layman can instantly understand and appreciate esthetically and which really do seem like 'it's thinking'. You couldn't ask for a better demonstration of the power and creativity of machine learning. (Although maybe OP is thinking more in a mathematical elegance vein by 'machine learning is beautiful'.)", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": [{"author": "ajmooch", "id": "dz6wwmr", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6wwmr/", "content": "I also like it from the point of view of the question,\n\"How far removed is what we consider 'true intelligence' from the simple mathematical tricks we employ to do ML today?\"\n\nIf you had never seen or heard of style transfer, and someone asked you \"On a scale of linreg to AGI, how difficult is it to paint a picture in the style of an artist? Of any artist?\"\n\nThe fact that this can be passably done with some gram matrix trickery and a simple optimizer, well, I find that beautiful.", "score": 15, "likes": 0, "upvote": 15, "downvote": 0, "comments": [{"author": "techlos", "id": "dzr9es0", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzr9es0/", "content": "what i find even more fascinating is the fact that you don't even need gram matrices - different statistical measures give different aspects of the style.\n\nUsing the channel mean & std also conveys stylistic information, however it's the more generalized, patterned information. Using histogram matching is incredible for replicating small details. Essentially, the style can be summarized as being the overall distribution of feature map activations, while the content is captured within the feature maps themselves. Realizing this opened my mind up to so many more possibilities for machine learning in terms of artistic uses.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "ginsunuva", "id": "dz7gkfu", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7gkfu/", "content": "The Gram matrix was something done by some guy named Bela something in the 90s I believe. His paper was on the types of statistics of texture.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "ispeakdatruf", "id": "dzjnizf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzjnizf/", "content": "Are you thinking of Bruno Levy? http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.857.1858&rep=rep1&type=pdf\n", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "BrotaroKubro", "id": "dz7ij20", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7ij20/", "content": "The paper on the [NEAT](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf) algorithm is my favorite ML/NN paper, and one I've recommended for friends when this kinda thing comes up. There are so many cool articles out there though. ", "score": 22, "likes": 0, "upvote": 22, "downvote": 0, "comments": [{"author": "tasubo", "id": "dzksd6f", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzksd6f/", "content": "I find it prettty cool that NEAT started getting some proper coverage. I remember when I first presented NEAT in Uni around 10 years ago and since that time basically I've never seen any mainstream research that would reference this work until like last year. I considered that to be extremely strange as the paper and research itself was amazing.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "ReallyBoringPerson", "id": "dzdt66k", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzdt66k/", "content": "It's a pretty neat paper", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "mynameisvinn", "id": "dzb015z", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzb015z/", "content": "good choice! ken stanley is one of the more original and thoughtful researchers. ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "aidan_morgan", "id": "dzbs2h8", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzbs2h8/", "content": "Yes, Stanley really did some amazing work with NEAT", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "Eymrich", "id": "dzxxuyb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzxxuyb/", "content": "I only work on ML in my free time, started 2 years ago. NEAT was the first thing I tryed to implement, and talking with friends about it was always cool and interesting, even for outsiders.\nSo it's the same for me \ud83d\ude00", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "nishankatwork", "id": "e0ha1qr", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0ha1qr/", "content": "[PAPERS 101 - How An AI Learned To See In The Dark?](https://medium.com/click-bait/papers-101-how-an-ai-learned-to-see-in-the-dark-d05fa1d60632?source=linkShare-4dc3d3b2cdec-1528716404) ", "score": 20, "likes": 0, "upvote": 20, "downvote": 0, "comments": [{"author": "Sau001", "id": "e1rpqj2", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1rpqj2/", "content": "Wow! ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "deepfiz", "id": "e3avagm", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3avagm/", "content": "Wow! ", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "halffloat", "id": "dz71ikn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz71ikn/", "content": "\"Rapid Object Detection using a Boosted Cascade of Simple Features\"\n\nhttps://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf\n", "score": 16, "likes": 0, "upvote": 16, "downvote": 0, "comments": [{"author": "qiyu_cs", "id": "dzh56zm", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzh56zm/", "content": "Me too. Very nice paper and easy to follow. I really like the evaluation part of the paper.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}, {"author": "prashant-kikani", "id": "e1rrhct", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1rrhct/", "content": "Anything related to health is beautiful for everyone. Bioinformatics is convergence of Biology & ML.\n\nI will show how ML have helped doctors (ultimately humans) to save lives.\n\nAt the end of the day, beauty is, how technology helped to improve quality of human life & on how many faces ML brings smile because of its power of understanding data.", "score": 15, "likes": 0, "upvote": 15, "downvote": 0, "comments": [{"author": "galwayhooker", "id": "e35e4k5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e35e4k5/", "content": "hear hear", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "_mulcyber", "id": "e1yqjmq", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1yqjmq/", "content": "Do you have any example papers/sources?\n\nI would love to get into Bioinformatics but I didn't go much further than Medical Imaging.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "yetipirate", "id": "e24ybwb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e24ybwb/", "content": ">I would love to get into Bioinformatics but I didn't go much further than Medical Imaging.\n\nyou stopped before the fun part :) genomics especially is challenging and has lots of room for improvement. If you take a close look at any of the papers published in nature or w/e you will see there are some pretty big flaws and limiting factors. worth a look imo.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "prashant-kikani", "id": "e1zahkv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1zahkv/", "content": "Sure. Here are some links. \n\nOne of the latest material I found is this.\nOpportunities and obstacles for deep\r\nlearning in biology and medicine\nhttp://rsif.royalsocietypublishing.org/content/15/141/20170387\n\nDeep learning for computation Biology\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4965871/\n\nThese links can be found by just Google search..\nThere are tons of material out there to learn.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "_mulcyber", "id": "e1zuayx", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1zuayx/", "content": "Thanks! ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}, {"author": "michaeltrs", "id": "e3c7xn4", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3c7xn4/", "content": "Bahdanau's attention paper [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473). A simple modification enabling neural nets to focus selectively on different parts of the data, beautiful!", "score": 16, "likes": 0, "upvote": 16, "downvote": 0, "comments": [{"author": "shortscience_dot_org", "id": "e3c7y24", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3c7y24/", "content": "I am a bot! You linked to a paper that has a summary on ShortScience.org!\n\n**Neural Machine Translation by Jointly Learning to Align and Translate** \n\n*Summary by Abhishek Das*\n\nThis paper introduces an attention mechanism (soft memory access)\n\nfor the task of neural machine translation. Qualitative and quantitative\n\nresults show that not only does their model achieve state-of-the-art BLEU\n\nscores, it performs significantly well for long sentences which was a\n\ndrawback in earlier NMT works. Their motivation comes from the fact that\n\nencoding all information from an input sentence into a single fixed length\n\nvector and using that in the decoder was probably a bottleneck. Inste... [[view more]](http://www.shortscience.org/paper?bibtexKey=journals/corr/BahdanauCB14)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "AbheekG", "id": "e43ldbc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e43ldbc/", "content": "Good bot!", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}]}]}, {"author": "2ros0", "id": "e3hl975", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3hl975/", "content": "the dropout layer! such a simple yet effective concept -- wish my brain had dropouts [http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)", "score": 15, "likes": 0, "upvote": 15, "downvote": 0, "comments": [{"author": "sigmoidp", "id": "e504ovw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e504ovw/", "content": "you can add dropout, it's called beer.\n\n&#x200B;", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "DonnyCrystal", "id": "e437w3q", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e437w3q/", "content": "Of course you do. Or you won't forget anything.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "None", "id": "e3j5hgp", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3j5hgp/", "content": "unused neurons tend to wither / growth occurs proportional to use.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "moewiewp", "id": "e4gc9iw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4gc9iw/", "content": "your brain got a pruning mechanism - which is like a permanent dropout mechanism with drop rate really really close to 1", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "serge_cell", "id": "e4i6vho", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4i6vho/", "content": "AlphaGo Zero", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": 0}, {"author": "XalosXandrez", "id": "dz6gthn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6gthn/", "content": "This reinvention of variational inference by Geoff Hinton is a delight to read \\- [Keeping the neural networks simple by minimizing the description length of the weights](http://svn.ucc.asn.au:8080/oxinabox/Uni%20Notes/honours/Background%20Reading/hinton1993keeping.pdf) .\n\nI'm a big fan of the \"bits\\-back\" argument and the whole communication theory viewpoint \\(minimum description length\\) of regularization.", "score": 26, "likes": 0, "upvote": 26, "downvote": 0, "comments": [{"author": "None", "id": "dz6hj6a", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hj6a/", "content": "[deleted]", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "yetipirate", "id": "dz6ky1p", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6ky1p/", "content": "It's rly popular! (Lasso anyone?) It's also undecidable to find an algorithm that solves for the minimum description length.  ", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": [{"author": "yldedly", "id": "e16faet", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e16faet/", "content": "> It's also undecidable to find an algorithm that solves for the minimum description length. \n\nThat's very interesting, do you have a link?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "yetipirate", "id": "e18ivuc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e18ivuc/", "content": "no link on hand, look up kolmogorov complexity.\n\nThe idea is simple, if you have some algorithm *A(l)* that can find the minimum length descriptor of some language, *l* then you can devise a language *B* whose MDL is of length |*A*| + c, where c is some constant. You then could describe B using *A(B)* thus achieving a description shorter than *B*'s MDL. A contradiction.\n\nSomething like that.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}]}, {"author": "question99", "id": "e06alpf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e06alpf/", "content": "I finished reading the paper now but I only understood around 50% of it. The parts where they are invoking statistical mechanics are increasingly challenging (maybe just for me as I have very little experience with the topic).", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "sieisteinmodel", "id": "e1m2nns", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1m2nns/", "content": "Why reinvention? Which other paper would be the invention paper that predates this one?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "piconzaz", "id": "dzthnmy", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzthnmy/", "content": "ML is a vast field. I'll let you choose the \"sub-field\" you're interested in and will rather focus on some authors who produce beautiful research and write their papers very well. When you read those guys, you definitely ARE smarter after.\n\nLeon Bottou: Amazing use of sophisticated mathematical tools in a very gentle way. ([The Trade-offs of Large Scale learning](https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf) probably still is my favorite paper. [Wasserstein GAN](https://arxiv.org/abs/1701.07875) is pretty amazing too)\n\nSanjeev Arora: To me, he is \"the guy\" that has been pushing the limits of the theoretical understanding of ML and DL, in the recent years, with tools that AFAIK were not known in the community.\n\nMax Welling: I used to have a low tolerance for the Bayesian church, but always had a soft spot for this guy. Amazing work on Variational Inference and relating it to stochastic optimization.", "score": 12, "likes": 0, "upvote": 12, "downvote": 0, "comments": [{"author": "shortscience_dot_org", "id": "dzthnz1", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzthnz1/", "content": "I am a bot! You linked to a paper that has a summary on ShortScience.org!\n\n**Wasserstein GAN** \n\n*Summary by MarvMind*\n\nThis very new paper, is currently receiving quite a bit of attention by the [community]().\n\n\n\nThe paper describes a new training approach, which solves the two major practical problems with current GAN training:\n\n\n\n1) The training process comes with a meaningful loss. This can be used as a (soft) performance metric and will help debugging, tune parameters and so on.\n\n\n\n2) The training process does not suffer from all the instability problems. In particular the paper reduces mode collapse significantly... [[view more]](http://www.shortscience.org/paper?bibtexKey=journals/corr/1701.07875)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "ipoppo", "id": "e19oiz4", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e19oiz4/", "content": "good bot", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "mutalk", "id": "e1jzyhf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1jzyhf/", "content": "I found 'Differentiable Neural Computers' by Deepmind fascinating. \nhttps://deepmind.com/blog/differentiable-neural-computers/", "score": 12, "likes": 0, "upvote": 12, "downvote": 0, "comments": 0}, {"author": "namescost", "id": "e1xn6ww", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1xn6ww/", "content": "A blank piece of paper, with a crayon, of course.\n\n(assuming they're equipped to understand it)", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": 0}, {"author": "fredtcaroli", "id": "e2ylyd5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ylyd5/", "content": "My vote goes out to Cycle GANs: [https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)  \n\n\nSuch a complex task \"solved\" with clever loss functions", "score": 12, "likes": 0, "upvote": 12, "downvote": 0, "comments": 0}, {"author": "melliKR", "id": "e3g0wep", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3g0wep/", "content": "The Wasserstein Auto-Encoder (WAEs) paper: [https://arxiv.org/abs/1711.01558](https://arxiv.org/abs/1711.01558). Super elegant generalization of + first theoretical justification for Adversarial Auto-Encoders (AAEs). ", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": [{"author": "qwertypi123", "id": "e3pzrp3", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3pzrp3/", "content": "Liked how this one tied together AAEs with a solid reason for their good performance", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "Devenar", "id": "e1kzcg5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1kzcg5/", "content": "[World Models](https://worldmodels.github.io/) (Here's [the paper](https://arxiv.org/abs/1803.10122)). They don't even have to have any knowledge about machine learning and it looks amazing.\n\nThe computer comes up with its own representation of games and then learns to play inside it's own \"dreams\" (to use the hype term)? That's beauty.", "score": 27, "likes": 0, "upvote": 27, "downvote": 0, "comments": [{"author": "thebackpropaganda", "id": "e2ayxfj", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ayxfj/", "content": "I'm curious to know what your background with ML is.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "Devenar", "id": "e2azefb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2azefb/", "content": "Just getting started. A couple years ago I learned the math behind feedforward neural networks, but struggled to use Tensorflow. Now that I\u2019ve found Keras, I\u2019m starting to put some ideas into practice to see what happens.\n\nIn short, I haven\u2019t read many papers, and when I read them, I usually don\u2019t understand all of the beauty of them. So this paper was special because the author built a website and you could actually understand what was going on, which I love.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "thebackpropaganda", "id": "e2b0y7r", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2b0y7r/", "content": "Thanks. So you liked the beauty of the presentation, not necessarily of the idea itself. That's fair.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "Devenar", "id": "e2b18ki", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2b18ki/", "content": "I\u2019d say I liked both. I think it\u2019s beautiful that a computer can come up with a simulation of how a world works and then learn to do things in the world based off of that simulation; I think it mimics the way humans plan a lot better than many of the other models, and I\u2019m excited for it to be applied to other more complex videogames so that APIs aren\u2019t needed.\n\nBut I think I appreciate the beauty of the presentation as well, because it allows me to actually see what the computer is seeing and how it plays games with itself (like making the balls disappear into thin air by making particular moves that trick the simulator) and learns.\n\nWhile the paper is nice and has a cool idea, I think there are many papers with lots of cool ideas that I don\u2019t have the time right now to understand. This one was great because I feel like anyone can understand its beauty. That part is due mostly to the website.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}]}]}, {"author": "pandeykartikey", "id": "e1ve895", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1ve895/", "content": "I would show him/her something like Neural Style Transfer ([https://arxiv.org/pdf/1508.06576.pdf](https://arxiv.org/pdf/1508.06576.pdf)).", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": [{"author": "Maciekism", "id": "e21vacn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e21vacn/", "content": "Why are you assuming it's a him? ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "pandeykartikey", "id": "e21vw9x", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e21vw9x/", "content": ">Why are you assuming it's a him?\n\nMy Bad", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "Maciekism", "id": "e2dy0cz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2dy0cz/", "content": "Wow I got so much hate for pointing this out. I wasnt even serious... I guess people really hate when you point out subconscious biases. ", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "PM_ME_UR_HPARAMS", "id": "e2f784p", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2f784p/", "content": "Technically \"he/him\" is gender agnostic when the context is not clear (a practice which predates the founding of the United States): https://en.wikipedia.org/wiki/Singular_they#Prescription_of_generic_he\n\nThen some people had to turn it into a social justice issue.\n\n/u/pandeykartikey", "score": 17, "likes": 0, "upvote": 17, "downvote": 0, "comments": [{"author": "Maciekism", "id": "e2h9zhv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2h9zhv/", "content": "Not that i think its such a big deal in this example but the langue we speak does heavily influence our perception of the world. I am a bit allergic to certain social justice issues and people do take it too far all of the time (history rather then herstory for example) But in this case he/him became the common pronouns because of male dominance in the old world. Also people dont use he because they are aware of it being gender-less people use it because they are simply assuming a gender when having a thought. (at least majority of the time and as it was in this example proven by what he wrote down below) This isnt a tragedy but it does point to the fact that people do have unwarranted subconscious bias", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "PM_ME_UR_HPARAMS", "id": "e2hfwvw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2hfwvw/", "content": "Actually, the discussion of \u201che or she\u201d was brought up when the first grammar books were written. They decided against it because in several contexts the addition of the 2 words changed sentence structure and was clumsy. (One of those first grammar books was written by a female, by the way.)\n\nYour claim that there are more than 2 genders (one that I don\u2019t necessarily agree with but I\u2019ll play along) only serves to reinforce my point.\n\nShould we be writing our sentences as \u201che or she or xe or they or heshe...\u201d as each gender is invented? Am I \u201csexist\u201d if I don\u2019t include however genders there God-knows-how-many are?", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": [{"author": "Maciekism", "id": "e2huo5v", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2huo5v/", "content": "I never claimed that there are more then 2 genders. I simply stated that people that use he/him dont usually use it because they are aware its gender neutral pronoun. Which actually in some cases it isnt so it makes things confusing and makes you really heavily on context. \n\nI also didnt call anyone a sexist. Having a subconscious bias doesnt necessarily mean youre a sexist. if it would then everyone would be sexist because everyone has a subconscious bias one way or another. In my opinion i wouldnt call someone a sexist unless their belief is a conscious one. If they actually think females are inferior then I would use the word sexist to describe them. Additionally being a female does not exclude you from being sexist against other females- Or holding believes that sexism is not harmful.\n\nI do not think it makes grammar awkward. Just use them/they/their", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": [{"author": "PM_ME_UR_HPARAMS", "id": "e2ivho8", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ivho8/", "content": "> them/they/their\n\nWhich are all plural pronouns and imply multiple individuals when it is not necessarily the case. This would be incorrect, grammatically. You'd probably miss points on the SAT for it.\n\nEven people who share your beliefs about gender advocate for \"he or she\".", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}]}]}]}]}]}]}]}, {"author": "gohu_cd", "id": "dz6ds5q", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6ds5q/", "content": "It's not a paper but the kernel trick for SVM is what got me into ML, because I thought it was a very elegant and clever trick.", "score": 33, "likes": 0, "upvote": 33, "downvote": 0, "comments": [{"author": "willis77", "id": "dz6p7ev", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6p7ev/", "content": "http://oneweirdkerneltrick.com/", "score": 16, "likes": 0, "upvote": 16, "downvote": 0, "comments": 0}, {"author": "hlynurd", "id": "dz6enoo", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6enoo/", "content": "[http://www.eric\\-kim.net/eric\\-kim\\-net/posts/1/kernel\\_trick\\_blog\\_ekim\\_12\\_20\\_2017.pdf](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick_blog_ekim_12_20_2017.pdf)", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": 0}, {"author": "claytonkb", "id": "dzzzgi5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzzzgi5/", "content": "For me, it was the hashing trick (similar motivation to probabilistic data structures like Bloom filters) -- here's [a paper](https://arxiv.org/abs/1504.04788) that takes the hashing trick to the next level.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "ApertureCombine", "id": "dz7l6jt", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7l6jt/", "content": "Granted I haven't read that many papers, but I find [this](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) paper on word2vec really cool. Being able to extract various complex relationships between words just from a general corpus of text is incredibly interesting.", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": [{"author": "ai_math", "id": "e1908ff", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1908ff/", "content": "[This paper](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf) which connects the skip-gram negative sampling model if word2vec to factorization of PMI matrices is one of my favorites.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "holt0102", "id": "e3bo7ad", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3bo7ad/", "content": "Something about AutoEncoders. Always found the idea fascinating.", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": [{"author": "gunthercult28", "id": "e3dinol", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3dinol/", "content": "I'm excited by the prospect of using autoencoders for imputing features, effectively using them to clean dirty databases.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "holt0102", "id": "e3drl7i", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3drl7i/", "content": "Could you elaborate on that ? It sounds interesting. \n\nLike automatically cleaning databases training the \u201cCleaner\u201d with good normal entries of data, to fix the bad ones ?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "gunthercult28", "id": "e3emsry", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3emsry/", "content": "100.\n\nThe autoencoder acts to normalize the data down to smaller feature space. It's not guaranteed to perfectly return the resulting image of grandma you input, but rather subtle differences should come out.\n\nIn this case, we want to leverage those subtle differences to generally impute values based on the topology of our feature space. So rather than declare an imputer per feature, we declare a network that encodes what invoice data topology at Company XYZ should look like in our database and fixes errors.\n\nThis is just me imagining. I don't have the practical knowledge to build and train this kind of network, but theoretically it can learn to encode arbitrary numerical spaces to a subspace, and further it acts as a function transforming an input object to almost itself as an output object through that subspace transformation.\n\nAllowing for corrections is essentially a threshold for acceptible error between input and output data.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}]}]}]}, {"author": "evc123", "id": "dz6gwvd", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6gwvd/", "content": "Translating Neuralese\n\nhttps://arxiv.org/abs/1704.06960", "score": 28, "likes": 0, "upvote": 28, "downvote": 0, "comments": [{"author": "twocatsarewhite", "id": "dz6jxjt", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6jxjt/", "content": "Anything by that guy really? Speaking of which, how the heck does he regularly publish 3+ high quality work a year?", "score": 17, "likes": 0, "upvote": 17, "downvote": 0, "comments": [{"author": "sensual_onlooker", "id": "dz6p396", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6p396/", "content": "May I know whom you are referring? It's a 3 author paper. Im into AI now. I need a strong base to compound my knowledge. I thought this sub consensus is enough to pick a virtual mentor.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "MTGTraner", "id": "dz6p90e", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6p90e/", "content": "The first author is the default reference.", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": [{"author": "sensual_onlooker", "id": "dz6pook", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6pook/", "content": "Thanks", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "twocatsarewhite", "id": "dz7mpo3", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7mpo3/", "content": "Yeah, Jacob Andreas is who I meant as well.\n\nEdit: Typo", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}, {"author": "clifgray", "id": "e1q56y2", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1q56y2/", "content": "This sounds eerily like what a robot trying to learn more about machine learning would say.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "sensual_onlooker", "id": "e1q6d6w", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1q6d6w/", "content": "Thanks human\n\n^^beep ^boop ^Iam ^^a ^^bot\n\n", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}, {"author": "terrorlucid", "id": "e06o56v", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e06o56v/", "content": "how many years has he done that?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "rlrgr", "id": "dz6szsc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6szsc/", "content": ">To obtain human annotations for this task, we recorded both actions and messages gener- ated by pairs of human Amazon Mechanical Turk workers playing the driving game with each other. We collected close to 400 games, with a total of more than 2000 messages exchanged, from which we held out 100 game traces as a test set.\n\nSomething about this just blows my mind. The human race is farming itself for intelligent behavior... with the goal of improving (or perhaps replacing) itself.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "phobrain", "id": "dz7gql5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7gql5/", "content": "I'm mining myself for a sort of unintelligent behavior (think Rorschach test), to create a sort of self-awareness training to help humanity stay on top of things. You're welcome! :-)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "proimprobable", "id": "e26mwbm", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e26mwbm/", "content": "Surprised all top answers are for very recent papers. Anyway, I personally find fascinating the connection of physics to ML, and Neal's paper on HMC is such an intuitive and clever paper in that category.", "score": 9, "likes": 0, "upvote": 9, "downvote": 0, "comments": 0}, {"author": "nachiket273", "id": "dz6yw8b", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6yw8b/", "content": "Original paper of [GAN](https://arxiv.org/pdf/1406.2661.pdf) . ", "score": 18, "likes": 0, "upvote": 18, "downvote": 0, "comments": 0}, {"author": "jamsawamsa", "id": "e2nks25", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2nks25/", "content": "Mind would have to be [Imagination Augmented Agents for Deep RL](https://nurture.ai/papers/imagination-augmented-agents-for-deep-reinforcement-learning).\n\nI love how some of the most novel methods that people have come up with to train RL agents end up being so similar to how the human brain learns as well. And to bring in \"imagination\" into it's training. \n\nJust love the concept.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "gagablob", "id": "e3t4ukf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3t4ukf/", "content": "Do we know how the human brain learns?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "jamsawamsa", "id": "e3t5l62", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3t5l62/", "content": "Neuroscience and behavioral psychology studies mate. I think the first chapter of the Barto n Sutton book gives a good intro to how reinforcement learning came about ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "gagablob", "id": "e3t6gjv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3t6gjv/", "content": "The intricacies of brain are still unknown and neuroscientist are yet to understand all about how human brain learns. And isn't reinforcement learning engineered just based on the concept of pavlovian reinforcement learning?", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "jamsawamsa", "id": "e3t7qjs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3t7qjs/", "content": "Yeah, you are right. RL models the concept of pavlovian RL. Guess it's my wording I got to work on in the original statement.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}]}]}, {"author": "Tsadkiel", "id": "e3v9v63", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3v9v63/", "content": "Stacked GANs.  Put in a text description.  Get a birb", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}, {"author": "stgstg27", "id": "e449547", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e449547/", "content": "Paper on GAN. The idea that adversarial networks leading to better generation is beautiful in itself \n\n", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": [{"author": "nobodykid23", "id": "e481reo", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e481reo/", "content": "Don't forget the following GAN tutorial. It's a good intro for someone who want to start diving into GANs", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "deluded_soul", "id": "e4cejah", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4cejah/", "content": "Can you share the link? ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "nobodykid23", "id": "e4cvgyj", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4cvgyj/", "content": "Here you go : [NIPS 2016 tutorial: Generative adversarial networks](https://arxiv.org/abs/1701.00160)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}]}, {"author": "shayanfazeli", "id": "e0i2ao1", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0i2ao1/", "content": "I would choose the paper \"Show, Attend, and Tell\".", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}, {"author": "jayjay59", "id": "e4fxguk", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4fxguk/", "content": "Link to a freemind mind map with links to all papers referenced in this article can be found  here https://imgur.com/a/UHde66q", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "imguralbumbot", "id": "e4fxh54", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4fxh54/", "content": "^(Hi, I'm a bot for linking direct images of albums with only 1 image)\n\n**https://i.imgur.com/73GUiYF.jpg**\n\n^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&subject=ignoreme&message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&subject=delet%20this&message=delet%20this%20e4fxh54) ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}, {"author": "min_sang", "id": "dz6ni5x", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6ni5x/", "content": "[Wavenet](https://arxiv.org/pdf/1609.03499.pdf) ", "score": 14, "likes": 0, "upvote": 14, "downvote": 0, "comments": [{"author": "geoffreygoines", "id": "e08omkp", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e08omkp/", "content": "[Parallel WaveNet](https://arxiv.org/pdf/1711.10433.pdf) is amazing too", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}]}, {"author": "DeepDreamNet", "id": "e1g7kzx", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1g7kzx/", "content": "I'd actually pick a paper that shows the strength of the community, specifically the Google technical debt paper - they worked hard to make it accessible, they know this stuff cause they operate at huge scale, and they put out a map for friend and foe alike to keep us from falling in the same potholes they did -- [https://ai.google/research/pubs/pub43146](https://ai.google/research/pubs/pub43146)", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "3pence", "id": "e369bfx", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e369bfx/", "content": "Rosenblatt's perceptron.\nIt is one of the most tangible and beautiful mathematics doing wonders examples there is. Linear Bound Domains aside.", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}, {"author": "TakoTabak", "id": "dzk3p9f", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzk3p9f/", "content": "This one\\-shot paper [https://arxiv.org/abs/1702.06559](https://arxiv.org/abs/1702.06559) started for me the idea of combining reinforment learning and superviced learning. GAN also does this but still I like the approach taken her.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "tomvorlostriddle", "id": "dzrjkcf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzrjkcf/", "content": ">superviced learning\n\nSounds dangerous", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": 0}]}, {"author": "diggerdu", "id": "dzxkv2h", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzxkv2h/", "content": "One big net for everything", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}, {"author": "CindicatorPusheens", "id": "e4njjn0", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4njjn0/", "content": "I would actually start some explanatory articles with examples and codes to people who don't have a lot of knowledge, but would understand ML better and may become interested in the field more\n\nLike this one [Music generation with Neural Networks\u200a\u2014\u200aGAN of the\u00a0week](https://medium.com/cindicator/music-generation-with-neural-networks-gan-of-the-week-b66d01e28200)", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "MysteriousBus5", "id": "e2cu7hp", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2cu7hp/", "content": "How one needs only two variables/features to know what the number is. This is pure magic.\n\n[https://i.stack.imgur.com/2gSs1.png](https://i.stack.imgur.com/2gSs1.png) ", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": [{"author": "SomeRandomGuydotdot", "id": "e2l7c60", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2l7c60/", "content": "I'm partial to a slightly different school.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)\n\nThe original paper regarding residual networks, and hella interesting if viewed from the pure math perspective, as they should be equivalent, but in practice are not. Really drives home that universal approximation is not the cure all.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n[http://cnnlocalization.csail.mit.edu/supp.pdf](http://cnnlocalization.csail.mit.edu/supp.pdf)\n\nProbably the most clear cut way of describing GAP-CNNs attention regions I've ever seen. CAMs changed how I understood features.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nAs for the paper I care about most in my work (Fuck it, couldn't find it but this is close):\n\n[https://www.ijcai.org/Proceedings/15/Papers/561.pdf](https://www.ijcai.org/Proceedings/15/Papers/561.pdf)\n\nThe key takeaway, is that CNNs can be applied to time series data, and out perform bag of features crafted by PhDs.\n\nThis implies to me, the general applicability of CNNs to many real world problems.\n\nThis is beautiful because it simplifies a whole class of hard problems, into simple coding and data collection. A more approachable work flow compared to running a major research department.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "shortscience_dot_org", "id": "e2l7cv0", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2l7cv0/", "content": "I am a bot! You linked to a paper that has a summary on ShortScience.org!\n\n**Deep Residual Learning for Image Recognition** \n\n*Summary by Martin Thoma*\n\nDeeper networks should never have a higher **training** error than smaller ones. In the worst case, the layers should \"simply\" learn identities. It seems as this is not so easy with conventional networks, as they get much worse with more layers. So the idea is to add identity functions which skip some layers. The network only has to learn the **residuals**. \n\n\n\nAdvantages:\n\n\n\n* Learning the identity becomes learning 0 which is simpler\n\n* Loss in information flow in the forward pass is not a problem a... [[view more]](http://www.shortscience.org/paper?bibtexKey=journals/corr/HeZRS15)", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "benevolentpirate", "id": "e2eocep", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2eocep/", "content": "Can you please explain? To-be-junior undergrad here. :)", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "MysteriousBus5", "id": "e2euxoo", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2euxoo/", "content": "Sure :) \n\nThe graphs are related to the dimensionality reduction. The experiment is try to reduce the dimensionality of MNIST dataset (set of images of hand-drawn digits) as much as possible without loosing their separability in the lower dimensional space. \n\nOn the right is algorithm PCA which reduces the dimension by eliminating the directions which have less variance and then projecting the data on the remaining dimensions. On the left is auto-encoder (think it like a neural network same number of nodes at input and output layers, but very few at the middle, 2 in this case) which feeds the image at input layer and expect the same image at output layer, but near the middle of the network, the number of layers are drastically reduced, thus creating a squeezing kind of process, or information bottleneck kind of phenomenon.\n\nThe magic is in the output. Consider the right image, all the colours are distributed across the 2-D space with no or very less overlap. Feel this as, you are like a magician, who is allowed to ask only two questions about the image, and based on answer, you'll be able to very well predict the number. Just two questions, or two features was enough to know the entire number.\n\nStill, didn't get the magic ?", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": [{"author": "Ciductive", "id": "e2fm2oj", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2fm2oj/", "content": "Shouldn't you theoretically be able to reduce it to only one dimension? The number itself?\n\nIf I'm only allowed to ask one question and I ask what the label/number is, isn't that clearly sufficient to know the number? ", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": [{"author": "None", "id": "e2gxjvd", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2gxjvd/", "content": "> Shouldn't you theoretically be able to reduce it to only one dimension? The number itself?\n\nTheoretically yes. Theoretically, you could have a projection that projects the data onto a real or natural number line. And with the right scaling, the points belonging to the 0 fall on 0, 1 on 1, etc.\n", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "Ciductive", "id": "e2hzba6", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2hzba6/", "content": "So why is this impressive then?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "MTGTraner", "id": "e2i0ww3", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2i0ww3/", "content": "Presumably because this is done in an unsupervised manner.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "Ciductive", "id": "e2i25pz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2i25pz/", "content": "What a stupid neural net haha, what kind of idiot needs two numbers to represent a single number haha", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": [{"author": "archxeon", "id": "e2i5as8", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2i5as8/", "content": "You are actually representing  28 x 28 numbers by two numbers.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "Ciductive", "id": "e2i5yxa", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2i5yxa/", "content": "When you should be able to do it with one...", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": []}]}]}]}, {"author": "None", "id": "e2ichwp", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ichwp/", "content": "well, I think you are fixated too much on the \"number\" as label. Instead of images of digits, just think of it as arbitrary objects, like house, ball, table, chair, etc. and then you maybe get a better feeling why grouping images via only two numbers is impressive", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}, {"author": "PM_ME_UR_HPARAMS", "id": "e2f6yae", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2f6yae/", "content": "> which reduces the dimension\n\nCould you not use SVD and select better principal components to accomplish the same thing as the autoencoder?\n\nAssuming you can use a hyperplane to divide each category.\n\nEDIT: Nvm I'm dumb I just realized that the magic is the autoencoder selected those very \"better principal components\" I was talking about.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "benevolentpirate", "id": "e2ezvbh", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ezvbh/", "content": "Wow. This is really cool!\nThanks for explaining! :)", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "PhysLane", "id": "e33ck8p", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e33ck8p/", "content": "I am curious if you can go into more detail about how the auto-encoder works.  Does it require normalization? (\\*)\n\n\\*If so then would it not be susceptible to the same issues PCA has with using Z-scores for normalization?  Or are they worse because of scaling of normalization affects computation time.  Conjugate Gradient Descients performance (computation time) is affected by normalization (called parameter scaling), so since so many networks are based on Gradient Descent, I wondered how much normalization affects the reduction.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}, {"author": "proptrot13", "id": "e2uvoea", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2uvoea/", "content": "So then you feed this data through a neural network and it must improve performance by a lot?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "alkalait", "id": "e3wb209", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3wb209/", "content": "I don't agree. Sure you've learned a mapping from (mnist) image space to 2D, but you still to retain the all the information that defines the mapping.\n\nSame deal with PCA: you need the principal components (aka loadings), in addition to the reduced-dimension coordinates (Score 1 & 2 in the figure), to fully describe a number's mnist image.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "mbox171", "id": "dzhwx87", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzhwx87/", "content": "YOLO", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "vannak139", "id": "dzm87g4", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzm87g4/", "content": "The Yolo v3 paper is, like v1 and v2, a gem. ", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}]}, {"author": "chrispher2012", "id": "e0h8ja2", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0h8ja2/", "content": "T-SNE, show me how to analysis the machine learning problem!", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "roeiherzig", "id": "e0haqgb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0haqgb/", "content": "My personal choice is \"[Relations Networks](https://arxiv.org/pdf/1706.01427.pdf)\" from DeepMind, which made a lot of impact on my recent work on Graph Networks for capturing Permutation Invariance on [Scene Graphs prediction](https://arxiv.org/pdf/1802.05451.pdf) (submitted to NIPS18 ).\n\nWhat so special on the DeepMind paper is that I think it tries to bridge the gap between DL and classic ML (probability graphical model). I really love that.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "geomtry", "id": "e1c0k3b", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1c0k3b/", "content": "Indeed this is a very exciting direction", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "russel_russel", "id": "e3vvufq", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3vvufq/", "content": "My vote goes to Style Transfer for Decorated Logo Generation: https://www.groundai.com/project/contained-neural-style-transfer-for-decorated-logo-generation/ ", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "rosivagyok", "id": "e4cri9t", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4cri9t/", "content": "Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural Networks\n\nhttps://arxiv.org/abs/1606.04393", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "randiscML", "id": "e4qm1yw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4qm1yw/", "content": "The original EXP3 paper. It is written so beautifully, I think it is fascinating what you can do with bandit feedback.", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}, {"author": "zhumao", "id": "dz6icav", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6icav/", "content": "Cybenko's [original paper](https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf) on the universal approximation property of superpositions of sigmoidal functions which:\n\n1. established the theoretical foundation of NN, moreover\n\n2. NN with one hidden layer neurons suffices \n\nSince then, works on NN are just tinkering, engineering, and hype e.g. deep learning.", "score": 14, "likes": 0, "upvote": 14, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz6xo9y", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6xo9y/", "content": "I'm a bit of a broken record on this topic, but polynomials satisfy the same property (see the Stone-Weierstrass theorem), so I guess by the same logic you could say that everything that's been done in ML or proto-ML for at least the last 70+ (maybe even 130) years is just 'tinkering, engineering, and hype.' \n\nThat's obviously absurd.", "score": 33, "likes": 0, "upvote": 33, "downvote": 0, "comments": [{"author": "zhumao", "id": "dz6zmbt", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6zmbt/", "content": ">That's obviously absurd.\n\nThe superposition/composition of sigmoidal function approximation only requires a single hidden layer neurons i.e. one composition, unlike polynomials used in S-W thoerem, no unknown higher power needed, nor restricted to compact domains.\n\nThe comparison is absurd.\n\nAs a final note, this result also \"rescued\" NN from Marvin Minsky's famous counter example on perceptron's inability to solved the XOR problem.", "score": -1, "likes": 0, "upvote": -1, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz70y7j", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz70y7j/", "content": "Instead of an unknown number of terms needed in your polynomial basis you need an unknown number of neurons in that single layer, and, yes, there is indeed a restriction to a compact domain (the unit hypercube) in Cybenko's paper.\n\nWhat I was calling absurd is the claim that everything since is just 'tinkering, engineering, and hype.' Cybenko's theorem has little bearing on whether neural networks are useful in practice. ", "score": 31, "likes": 0, "upvote": 31, "downvote": 0, "comments": [{"author": "zhumao", "id": "dz7200s", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7200s/", "content": ">Instead of an unknown number of terms needed in your polynomial basis you need an unknown number of neurons\n\nThat's where engineering comes in, and in comparison how much \"engineering\" effort is done since S-W theorem was proved, hence how useful was that in comparison? ", "score": -1, "likes": 0, "upvote": -1, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz72cvw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz72cvw/", "content": "I don't understand the question. ", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "zhumao", "id": "dz72pin", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz72pin/", "content": "How useful is S-W theorem in comparison, for example in ML i.e. was polynomials being seriously considered as an important tool?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz736lr", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz736lr/", "content": "I guess neither polynomial regression nor single layer neural networks are that important as practical tools in ML. \n\nIf, however, all that mattered were results about function classes being dense in the set of continuous functions restricted to compact domains, then they'd be equally important. But that's not all that matters. Neither S-W nor Cybenko's theorem say anything about learning from data, which is kind of critical in ML. \n\nAs a side note, there's a lot of theory related to kernel methods (of which polynomial regression is one instance) which is more pertinent to actually learning from data than S-W. ", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "zhumao", "id": "dz7834i", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7834i/", "content": "> then they'd be equally important. But that's not all that matters. \n\nNo, it's not equivalent, the body of work on NN dominates.\n\n>Neither S-W nor Cybenko's theorem say anything about learning from data, which is kind of critical in ML.\n\nBoth approximation theorems clearly proved otherwise as part of ML, except one is more useful than the other, so far.", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz78pmz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz78pmz/", "content": "You seem very confident, at least.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}]}]}]}]}]}]}]}]}, {"author": "GVR64", "id": "dzg85r9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzg85r9/", "content": "The proof of Cybenko is non constructive and uses Hahn -Banach beautifully. I do not know of any constructive proof using the Hahn -Banach to construct an approximation as formulated in Cybenko paper. It is a beautiful paper but to call other NN works as tinkering and hype is very harsh and unfair. You can justify your stance a bit, if you work in the field of Convexity and Optimisation and feel neglected about the relative lack of spotlight on your area compared to Deep learning. With waning public funding and interest the only way we can fund and pursue Hahn Banach like gems is through generating hype and memes like these. If everything is only about proving existence elegantly then we may as well ask \"Who is this Magellan person anyway?\" ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "zhumao", "id": "dzgdmr9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzgdmr9/", "content": ">The proof of Cybenko is non constructive and uses Hahn -Banach beautifully. I do not know of any constructive proof using the Hahn -Banach to construct an approximation as formulated in Cybenko paper.\n\nIndeed, the proof is dope but not sure the non-constructive argument is a blemish. Non-constructive proofs, and many are beautiful, is rife in math, and not just in analysis e.g. the fundamental theorem of algebra, the number of primes is infinite, etc.\n\n>It is a beautiful paper but to call other NN works as tinkering and hype is very harsh and unfair. You can justify your stance a bit,....\n\nThe result also provided an explicit expression (though not fixed in number of hidden layer neurons) to formulate the problem as well as rescued NN from Marvin Minsky's famous/infamous example of the limitation of [NN/perceptrons](https://en.wikipedia.org/wiki/Perceptrons_(book). \n\nOTOH, do plead guilty on the tone, I should've added \"applications\". As for \"hype\", there are plenty, and here is a a typical [example](https://arxiv.org/abs/1608.08225) (which I find the level of hype and the vacuousness in content has reached to a new height), from the *New Physics* crowd no less. \n\n>you can justify your stance a bit, if you work in the field of Convexity and Optimisation and feel neglected about the relative lack of spotlight on your area compared to Deep learning. With waning public funding and interest the only way we can fund and pursue Hahn Banach like gems is through generating hype and memes like these. If everything is only about proving existence elegantly then we may as well ask \"Who is this Magellan person anyway?\"\n\nPublic funding is not an issue here since not in academia, I work in applications of optimization, and mostly non-convex problems and NP-hard problems which included using NN as a tool.\n", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "cognizant_ape", "id": "dzi149p", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzi149p/", "content": "I like to point out that the proof was not only non\\-constructive but it didn't establish the learnability of such functions, just their existence.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "p_pistol", "id": "dz6rfrn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6rfrn/", "content": "Yup, that's what I was thinking too.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "sweatyCameltoe", "id": "dz7kqzs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7kqzs/", "content": "Nvidia's end to end learning for self driving cars https://arxiv.org/abs/1604.07316\n\nIt has some feature visualisations so you can \"see\" something and there are nice videos of it working. With only a 100hours of driving data.. This was my eye opener", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "deck13", "id": "dze6nxb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dze6nxb/", "content": "You may have to read between the lines:\n\nhttp://www.ams.org/journals/bull/2018-55-01/S0273-0979-2017-01597-2/home.html", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "cucfufofo", "id": "dzp8yiy", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzp8yiy/", "content": "A Unifying Review of Linear Gaussian Models", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "guugvidil", "id": "dzp9lvz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzp9lvz/", "content": "A Unifying Review of Linear Gaussian Models", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "udnaan", "id": "e28oaov", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e28oaov/", "content": "You can't. Unless if they are already heavily invested into programming or mathematics. You can however show them the by products and application of machine learning.\n\nPick your favorite but I usually go with VAEs (such as my profile image)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "_10ZIN_", "id": "e552kh9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e552kh9/", "content": "**Text to image synthesis using GAN**\n\nThe title of the paper itself is so riveting, it for sure calls upon an exhilarating  read.\n\nshout out to Ian Goodfellow for inventing GAN, to make deep learning all the more special.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "battboe", "id": "dz6wry5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6wry5/", "content": "Old school vision paper that got started.. [Modeling the shape of the scene: a holistic representation of the spatial envelope](http://people.csail.mit.edu/torralba/code/spatialenvelope/)", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "jem-mosig", "id": "dz7etaz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7etaz/", "content": "I love this one, making a fascinating link to physics: \"Why does deep and cheap learning work so well?\" https://arxiv.org/abs/1608.08225", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "efavdb", "id": "e0qbtev", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0qbtev/", "content": ">A Unifying Review of Linear Gaussian Models\n\nAgree, it includes a lot of interesting insight that makes dl seem less magical to me.  Whether the ideas there actually how things work is another question.. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "None", "id": "e2akv1x", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2akv1x/", "content": "[deleted]", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "thebackpropaganda", "id": "e2azc6u", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2azc6u/", "content": "Can't agree more. I can't wait for UPN++. This is probably the kind of model that if scaled up could lead to AGI.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "PuzzledForm", "id": "e2b5on1", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2b5on1/", "content": "Isn't David Hardmaru's World Models = AGI? ", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}, {"author": "Majesticeuphoria", "id": "dz6etej", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6etej/", "content": "[This one!](https://arxiv.org/abs/1802.08195)", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "inkplay_", "id": "dz6kksi", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6kksi/", "content": "BEGAN I a mean just look at their results and began thinking about the possibilities...", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "Starkll7", "id": "dzwcjt3", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzwcjt3/", "content": "The alpha zero paper would definitely be my pick", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "kmc5500", "id": "e3row4h", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3row4h/", "content": "I totally agree this one. No paper had bigger impact to public. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "PucheroDelAverno", "id": "e1k78a5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1k78a5/", "content": "I wonder what are the best scientific discoveries made by machine learning.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "None", "id": "e2xqyzs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2xqyzs/", "content": "[removed]", "score": -4, "likes": 0, "upvote": -4, "downvote": 0, "comments": 0}]}, {"author": "olaf_nij", "id": "9bfr9p", "title": "[D] Having a more discussion focused MachineLearning subreddit", "url": "https://www.reddit.com/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/", "viewcount": null, "content": "After discussing with the other moderators, we are deciding to try out a new format for this subreddit. To encourage more focused discussions and a more positive community overall, we are going to emphasize the use of self-posts. Users can still post links, but only within self-posts to give the link more context for further discussion. Current exceptions to this with be arxiv.org links as we've seen in the past this already creates a fairly focused and positive discussion.\n\nWe're looking forward to seeing how these changes will continue to move the subreddit forward as it grows.", "upvote": 174, "downvote": 0, "upratio": 0.94, "subreddit": "r/MachineLearning", "comments": [{"author": "KingPickle", "id": "e5365kt", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5365kt/", "content": "I'm skeptical about this change for a few reasons:\n\n1. This sub (compared to others I'm on) really doesn't seem to suffer from a high volume of memes, shit-posts, etc. If anything, I think most of the dubious posts here *are self-posts* from newbies asking things google could answer.\n\n2. Putting all of the links inside of posts is an objectively worse UX. It requires an extra click/load process and it doesn't give you a thumbnail.\n\n3. As silly as it is, redditors like karma. And self-posts give you none. I doubt that will deter people posting links to videos/etc to academic experiments, etc. But I do wonder how much it will deter casual posters who might link a \"Two minute papers\" video (or something similar) that I'd find interesting.\n\nI'm 100% with you on the notion of fostering more discussion. I'd love that! But I'm not sold on this being the optimal path to encourage that.", "score": 56, "likes": 0, "upvote": 56, "downvote": 0, "comments": [{"author": "programmerChilli", "id": "e53fihh", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53fihh/", "content": "I think self posts do give you karma now; they changed that maybe a year ago.", "score": 19, "likes": 0, "upvote": 19, "downvote": 0, "comments": 0}, {"author": "tfburns", "id": "e53twyi", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53twyi/", "content": "1. I agree that some self-posts are from lazy newbies, but that's an argument for rule changes on self-posts, not on links (which have their own problems).\n\n2. This can be partially minimised via topic flairs and descriptive titles.\n\n3. Self-posts attract karma. But, you're right, people who like to post links are less inclined to put the effort into writing a self-post and linking within that.\n\nI'll be interested to see how this sub changes with the rule-change, but I don't think it was very necessary from my experience. I suspect we'll miss some decent blog posts and video links which could have generated interesting discussion.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "call_me_arosa", "id": "e53iajr", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53iajr/", "content": "I agree with you that it is worse UX.\nBut I don't necessarily see the point 3 as a bad thing.  \nFor example, the posts with 2 minutes paper links will now need to be inside a self post, ideally with a discussion or comment about the content.  \nFor me, one of the most disappointing aspects of r/ml as of now are links to contents with low quality and 0 significant discussions/insights about the topic in the post.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "abhishkk65", "id": "e52px0b", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52px0b/", "content": "I was curious how many posts this would affect so did a little analysis. Of the top 50 posts on the subreddit currently:\n\n* 22 are self posts\n\n* 13 are arxiv links\n\n* 3 are youtube links\n\n* 2 are twitter links\n\n* Most of the remaining 10 are blog posts.\n\nAssuming this is representative of the subreddit, this change will affect ~1/4 of future posts.\n\nMy initial reaction to this is positive, but I hope this doesn't prevent people from sharing interesting videos/academic blog posts because they feel like they have to add their commentary to it.", "score": 59, "likes": 0, "upvote": 59, "downvote": 0, "comments": [{"author": "AvatarUltima7", "id": "e52tqqp", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52tqqp/", "content": "I like it. Having some discussion and context is always super helpful before digging into the content. ", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": [{"author": "tfburns", "id": "e53u0qb", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53u0qb/", "content": "Isn't the title usually descriptive enough, e.g. \"Blog post on application of RNNs on XYZ\"?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "rlrgr", "id": "e53wf5r", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53wf5r/", "content": "But then you just get a post with no comments and 5 points. There are a million blogs about RNNs so it wouldn't hurt to write 2 sentences about why this one is worthwhile, to encourage people to check it out and to open the discussion.", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}]}]}, {"author": "alexmlamb", "id": "e52v3xg", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52v3xg/", "content": "It sounds like 38/50 are links, which is closer to 80% of posts.  ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "abhishkk65", "id": "e52v7sk", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52v7sk/", "content": "I don't think you read the original post correctly. self posts and arxiv links are still allowed. This means 15/50 are links that will now be self posts in the future. That's 30% which is ~1/4 affected.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "jer_pint", "id": "e52zlju", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52zlju/", "content": "How hard would it be to do this retrospectively on the subs entire history? Legitimately curious at doing some actual historical analysis", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "ManyPoo", "id": "e5391vs", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5391vs/", "content": "Can we also model it with an LTSM?", "score": -4, "likes": 0, "upvote": -4, "downvote": 0, "comments": 0}]}]}, {"author": "its_ya_boi_dazed", "id": "e52tplc", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52tplc/", "content": "Personally I enjoy the drama that sometimes occurs when people get into heated discussions. Lots of good ideas get thrown around. I hope mods don\u2019t over regulate this sub until it\u2019s basically an echo chamber. ", "score": 24, "likes": 0, "upvote": 24, "downvote": 0, "comments": [{"author": "maltin", "id": "e533e3b", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e533e3b/", "content": "I agree. I was positively surprised when I launched the debate on [Deep learning and tabular data](https://www.reddit.com/r/MachineLearning/comments/9826bt/d_why_is_deep_learning_so_bad_for_tabular_data/) a while back and not only people engaged in a positive way, but the traction it gathered brought many interesting ideas to my attention that I was not aware. Overall a much better experience than the average link in the sub.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}, {"author": "Silver5005", "id": "e53ntkm", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53ntkm/", "content": "> I hope mods don\u2019t over regulate this sub until it\u2019s basically an echo chamber. \n\nSo basically reddit?", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}]}, {"author": "olBaa", "id": "e531f5t", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e531f5t/", "content": "A lot of text posts are simple questions. If you are changing the sub structure, what about restoring the questions thread that was around a year or two ago?", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}, {"author": "gdrewgr", "id": "e539l9v", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e539l9v/", "content": "This seems like the exact wrong response. I'd much rather see a link to a GOOD blog post or talk on youtube than another dumb question as a self-post.", "score": 11, "likes": 0, "upvote": 11, "downvote": 0, "comments": 0}, {"author": "needlzor", "id": "e53erin", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53erin/", "content": "The only issue I can see is that if someone wants to post a discussion a blog post or a non-arxiv paper, they would have no way of knowing whether an existing one has already been posted. It could easily be fixed by forcing a naming convention where one has to use the title of the blog post (for example) as the title of the reddit thread, but then the moderation team would have to enforce that (or write a bot that enforces it). ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "quit_daedalus", "id": "e530668", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e530668/", "content": "Maybe creating a weekly discussion thread will help with blog posts and twitter links?", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "i-heart-turtles", "id": "e53qcs9", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53qcs9/", "content": "Is the [arxiv.org](https://arxiv.org) links rule generalized  to arbitrary paper abstracts? Or does the abstract have to be hosted on arxiv? For example, would a post linking here have to be a self-post? [https://www.ijcai.org/proceedings/2018/392](https://www.ijcai.org/proceedings/2018/392)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "tfburns", "id": "e53ua0c", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53ua0c/", "content": "Not convinced this rule-change was needed. What's your timeline for review of this rule-change and what measures will you use to measure success/failure?", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "thatguydr", "id": "e52wlyf", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52wlyf/", "content": "I love this, provided that people are smart about how they bring up blog posts within self posts. There are some *excellent* blog posts (though very rare) posted here on occasion, and putting them within the context of a self post seems very appropriate. Still, I hope that nobody decides that this is some \"everyone use clickbait titles\" free-for-all.\n\nCan we report excessive clickbait? If so, then I am 100% on board for this change. If not, then it's about 70% overall, because it's obvious what will happen.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "BeatLeJuce", "id": "e5304pp", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5304pp/", "content": "We agree. Also things like distill.pub etc. are great. We limited ourselves to arxiv for now because it's the obvious choice. Please do keep posting and discussing interesting stuff, that's the whole point!\n\nClickbait: yes! please use the modmail to send us stuff, or report it via the \"report\" button, it's super useful! We can't promise to always agree with the reports (we do get a lot of them), but we'll do our best to keep the subreddit as nice a place as possible.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}]}, {"author": "tfburns", "id": "e53tsgz", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53tsgz/", "content": "I think you should expand your link exceptions list to include relevant academic journals, since not everyone publishes on arxiv.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "Prcrstntr", "id": "e54hvfe", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e54hvfe/", "content": "I thought this sub is already very impressive. It's certainly one of the more professional ones. It's nice to see lots of threads about actual problems, unlike other subs that are 90% 'how do i get started installing tensorflow'. It would be good for all those discussion threads have more than 10 comments. ", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "Sumgi", "id": "e52r35h", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52r35h/", "content": "Have a look at r/geopolitics, they have a similar restriction.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "jer_pint", "id": "e52zcn1", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52zcn1/", "content": "What about linking to helpful walkthroughs/tutorials/blog posts? I'm thinking about medium for example, also lots of individuals host their own blog and sometimes the articles are more than self sufficient.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "zindarod", "id": "e530sp3", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e530sp3/", "content": "This is a great idea. This sub was turning into another /r/programmng.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "LazyOptimist", "id": "e55n52m", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e55n52m/", "content": "I'm all for this experiment, let's see how this goes. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "psykocrime", "id": "e564eu0", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e564eu0/", "content": "This seems like a completely pointless change. Personally I do not support it.\n", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "rstoj", "id": "e579llt", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e579llt/", "content": "The number of highly-upvoted posts seems to have gone down since this change. \n\nHow long is the experiment, and what are the metrics you are optimising for? ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "AdditionalWay", "id": "e5a4fkg", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5a4fkg/", "content": "I love the idea. I've seen times where someone just posts and imgur link from research and it gets tons of upvotes, but a paper to the actual research itself doesn't. So 100% support this. \n\nHow does the submit a link work? If you don't have an arxiv link, the automod deleted it?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "adammathias", "id": "e5bcags", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5bcags/", "content": "Any idea why the content of this self-post was removed?\n\nr/MachineLearning/comments/9ck5wg/intuition_on_the_trick_of_reversing_input/\n\nIt happens to have been very much in the spirit of the news rules.\n", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "deepNeural", "id": "e5cqpkf", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5cqpkf/", "content": "I have to agree with you. Most people on reddit live in their parents basement and worship trump and all things stupid. I'm afraid having posts with real merit and discussion are not gonna work so well here with these freaking morons.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "CactusSmackedus", "id": "e5361bd", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5361bd/", "content": "Rules are bad don't make them", "score": -5, "likes": 0, "upvote": -5, "downvote": 0, "comments": 0}, {"author": "Majesticeuphoria", "id": "e53fz22", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53fz22/", "content": "How come no-one has made an AI to make their reddit posts? /s", "score": -3, "likes": 0, "upvote": -3, "downvote": 0, "comments": 0}]}, {"author": "cpury", "id": "9cot4d", "title": "[P] Teaching your browser where you're looking at with TensorFlow.JS (tutorial / project)", "url": "https://www.reddit.com/r/MachineLearning/comments/9cot4d/p_teaching_your_browser_where_youre_looking_at/", "viewcount": null, "content": "Recently, I've been toying around with TensorFlow.JS and wanted to build a quick prototype. The result turned out to be actually quite cool! With only a handful of training examples collected over a few minutes, your browser can get pretty decent accuracy in predicting where you are looking at on the page! There's no backend behind it whatsoever.  \n\n\nI wrote a tutorial here that builds the complete project from start to finish:  \n[https://cpury.github.io/learning-where-you-are-looking-at/](https://cpury.github.io/learning-where-you-are-looking-at/)  \n\n\nI also built a more complete version with additional features here:  \n[https://github.com/cpury/lookie-lookie](https://github.com/cpury/lookie-lookie)  \n\n\nHopefully this inspires others to give TFJS a try. I think it opens a lot of interesting possibilities, and has a very friendly Keras-like API.\n\nPlease leave your feedback and ideas :)", "upvote": 100, "downvote": 0, "upratio": 0.96, "subreddit": "r/MachineLearning", "comments": [{"author": "squarific", "id": "e5ccv0c", "url": "/r/MachineLearning/comments/9cot4d/p_teaching_your_browser_where_youre_looking_at/e5ccv0c/", "content": "This looks super cool!", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "sinefine", "id": "e5cmusr", "url": "/r/MachineLearning/comments/9cot4d/p_teaching_your_browser_where_youre_looking_at/e5cmusr/", "content": "I was just googling about pyGaze! This is so amazing! Thank you so much for the write-up! I don't know anything about Tensorflow, but wanted to see if I could learn a bit, and this is the perfect example to start with!", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "sainimohit23", "id": "e5cy3mj", "url": "/r/MachineLearning/comments/9cot4d/p_teaching_your_browser_where_youre_looking_at/e5cy3mj/", "content": "Hi op, I know some deep learning and tensorflow. But I don't know web development. What else do i need to learn to make practical programs on tensorflow.js. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "jidacah", "id": "e5cyxue", "url": "/r/MachineLearning/comments/9cot4d/p_teaching_your_browser_where_youre_looking_at/e5cyxue/", "content": "this is pretty cool", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "Ikuyas", "id": "e5cgkx1", "url": "/r/MachineLearning/comments/9cot4d/p_teaching_your_browser_where_youre_looking_at/e5cgkx1/", "content": "I've been looking into TensorFlow.js for the particular feature of its being js and run on the browser. So, the browser collects data on the fly and train on the fly and generate something put into the practice on the fly? ", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": 0}]}, {"author": "hardmaru", "id": "9cqjc3", "title": "[R] Gibson Env: Real-World Perception for Embodied Agents", "url": "https://arxiv.org/abs/1808.10654", "viewcount": null, "content": "", "upvote": 9, "downvote": 0, "upratio": 0.92, "subreddit": "r/MachineLearning", "comments": [{"author": "hardmaru", "id": "e5cjdh9", "url": "/r/MachineLearning/comments/9cqjc3/r_gibson_env_realworld_perception_for_embodied/e5cjdh9/", "content": "Project site: http://gibsonenv.stanford.edu/", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "arXiv_abstract_bot", "id": "e5cjdkp", "url": "/r/MachineLearning/comments/9cqjc3/r_gibson_env_realworld_perception_for_embodied/e5cjdkp/", "content": "Title: Gibson Env: Real-World Perception for Embodied Agents  \n\nAuthors: [Fei Xia](http://arxiv.org/search/cs?searchtype=author&query=Xia%2C+F), [Amir Zamir](http://arxiv.org/search/cs?searchtype=author&query=Zamir%2C+A), [Zhi-Yang He](http://arxiv.org/search/cs?searchtype=author&query=He%2C+Z), [Alexander Sax](http://arxiv.org/search/cs?searchtype=author&query=Sax%2C+A), [Jitendra Malik](http://arxiv.org/search/cs?searchtype=author&query=Malik%2C+J), [Silvio Savarese](http://arxiv.org/search/cs?searchtype=author&query=Savarese%2C+S)  \n\n> Abstract: Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, \"Goggles\", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.  \n\n[PDF link](https://arxiv.org/pdf/1808.10654)  [Landing page](https://arxiv.org/abs/1808.10654)", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "rstoj", "id": "9ckuhk", "title": "[P] Lazydata: scalable data dependencies for Python projects", "url": "https://www.reddit.com/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/", "viewcount": null, "content": "https://github.com/rstojnic/lazydata\n\nI've written this tool in frustration with the current tools to manage data dependencies. \n\nI used to manually upload/download/backup my ML data and models. This worked until I accidentally overwritten some models that took weeks to train. \n\nAfter that I started putting everything in git with git-lfs to make sure everything is preserved. But when I started working in a team our repository grow super-big and took ages to pull. So we gradually abandoned putting all data files to git-lfs...\n\nI made lazydata as a middle path: hashes of files are stored in a version-controlled configuration file, and these are automatically verified, versioned and tracked for all files you use in code. When you pull the repo you get the config file and when you run the code  files that are needed are seamlessly downloaded.  \n\nThis has so far worked well for us, so I thought of sharing it! \n", "upvote": 98, "downvote": 0, "upratio": 0.97, "subreddit": "r/MachineLearning", "comments": [{"author": "iacolippo", "id": "e5bdxto", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5bdxto/", "content": "I was looking for something like this for ages! Thank you", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}, {"author": "RowdyIsCool", "id": "e5bj4j8", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5bj4j8/", "content": "Have you looked into Data Version Control? I use it on a current project and am liking it so far.\n\nhttps://dvc.org/", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": [{"author": "rstoj", "id": "e5bjwd6", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5bjwd6/", "content": "Yep, we had a look at dvc as well - cool tool! My understanding is that it's the same as git-lfs, just the smudge/clean cycle is not done in `git pull` but on `dvc pull`. This didn't really solve the problem of bloated repositories for us, as we didn't want to have to manually pull individual files.   ", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}]}, {"author": "kailashahirwar12", "id": "e5bmjgs", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5bmjgs/", "content": "Interesting project. I will be glad to be contributing and using Lazydata. ", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "trnka", "id": "e5bqz9c", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5bqz9c/", "content": "Looks good! Right now we're using git-lfs connected to artifactory and it's been wonderful, but we know it won't scale all the way.\n\nAre there other ways to configure S3 as the backend? Like could I set it in code? I don't want to have to ensure that remote-add command is run in every environment.\n\nDoes it just pull the AWS credentials from env variables/etc using boto3 defaults?\n\nDo you know of any issues we might run into, trying this in a Jenkins pipeline for code+model deployment?\n\nAnything in particular you'd like to see tested?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "rstoj", "id": "e5bsd9h", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5bsd9h/", "content": "You only need to run `add-remote` only once to add the s3 bucket location to lazydata.yml. It just adds `backend: s3://youbucket/yourkey` to this file. \n\nYes, it picks up default AWS credentials in ~/.aws. You can also configure them with `aws configure` or just copy over the creds into ~/.aws. \n\nHaven't tried it with Jenkins yet, but if you run into any issue let me know and I'll get it fixed :)", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "trnka", "id": "e5bsl8r", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5bsl8r/", "content": "For what it's worth this is very timely - I was looking around for ways to connect git-lfs to S3 just this morning cause the artifactory configuration trips up all our new hires.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}, {"author": "-Rizhiy-", "id": "e5bq5jo", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5bq5jo/", "content": "So it basically just automatically pulls files if they are missing?\n\nWhy not just write a function that checks if LFS is downloaded and pull it if not?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "rstoj", "id": "e5bqpv4", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5bqpv4/", "content": "Yes, automatically pulls missing files, and also automatically tracks file versions from inside code. I guess it would be possible to turn off LFS smudging and then manually unsmudge files. I think it might interfere with LFS though - I think it wasn't designed to be used in that way. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "-Rizhiy-", "id": "e5buk10", "url": "/r/MachineLearning/comments/9ckuhk/p_lazydata_scalable_data_dependencies_for_python/e5buk10/", "content": "http://shuhrat.github.io/programming/git-lfs-tips-and-tricks.html :)\n\nUsed that setting and hasn't had much trouble since. Even when working with repositories of 100+GB LFS.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}]}]}]}]}