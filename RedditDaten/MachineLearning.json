{"submission": [{"author": "MTGTraner", "id": "8kbmyn", "title": "[D] If you had to show one paper to someone to show that machine learning is beautiful, what would you choose? (assuming they're equipped to understand it)", "url": "https://www.reddit.com/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/", "content": "", "upvote": 1024, "downvote": 0, "upratio": 0.98, "subreddit": "r/MachineLearning", "comments": [{"author": "svaisakh", "id": "dz6flte", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6flte/", "content": "The [InfoGAN](https://arxiv.org/abs/1606.03657) was the first paper which really opened my eyes to the potential of unsupervised learning.\n\nWith nothing but raw data, the model learned abstract concepts like 'rotation', 'width' and 'stroke-thickness'.", "score": 210, "likes": 0, "upvote": 210, "downvote": 0, "comments": [{"author": "CraftyColossus", "id": "dz6hgti", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hgti/", "content": "Yup, that is my current go to paper to describe how cool unsupervised learning is.", "score": 27, "likes": 0, "upvote": 27, "downvote": 0, "comments": 0}, {"author": "MyNameIsJonny_", "id": "dz8a9mc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz8a9mc/", "content": "That was fascinating thank you. It was also very easy to follow, even as a lowly stats undergrad.", "score": 14, "likes": 0, "upvote": 14, "downvote": 0, "comments": [{"author": "aunick2017", "id": "e18n8vv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e18n8vv/", "content": "What are the background topics needed to follow this? I have a stats background too, but I found it hard to follow", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "sciencethat", "id": "e0lmcm4", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0lmcm4/", "content": "Just came across this. I've been wanting to start reading research papers for a while now. I finally started reading through InfoGAN but found I'm lacking a lot of other things to I should probably know in advance. Could you recommend a starter ML paper for me to just get the hang of how research papers are actually written?", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "None", "id": "dz6vib9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6vib9/", "content": "[deleted]", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "zhangqianhui", "id": "e11r2nn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e11r2nn/", "content": "The distangled representation learning has been researched for a long time.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "BotPaperScissors", "id": "e3ol4ol", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3ol4ol/", "content": "Paper! \u270b We drew", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "jiamengial", "id": "dz6fxa7", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6fxa7/", "content": "[A Unifying Review of Linear Gaussian Models](http://mlg.eng.cam.ac.uk/zoubin/papers/lds.pdf)", "score": 80, "likes": 0, "upvote": 80, "downvote": 0, "comments": [{"author": "ccmlacc", "id": "dz6qicz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6qicz/", "content": "This paper is like poetry. My favorite by far!", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "jsnoek", "id": "dzn2fhd", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzn2fhd/", "content": "A classic!", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "mr_tsjolder", "id": "e11rt9w", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e11rt9w/", "content": "multidimensional pancakes...", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "Stone_d_", "id": "dz770rc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz770rc/", "content": "What a fantastic Reddit post. Love it", "score": 59, "likes": 0, "upvote": 59, "downvote": 0, "comments": 0}, {"author": "ReginaldIII", "id": "dz6ha44", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6ha44/", "content": "I have a fondness for [[Gatys et. al. 2015]](https://arxiv.org/abs/1508.06576)'s seminal work on neural artistic style transfer. There is a simplicity and elegance in the use of Gram matrices that made me want to understand how on earth they could convey stylistic similarity so well. ", "score": 101, "likes": 0, "upvote": 101, "downvote": 0, "comments": [{"author": "shortscience_dot_org", "id": "dz6hamg", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hamg/", "content": "I am a bot! You linked to a paper that has a summary on ShortScience.org!\n\n**A Neural Algorithm of Artistic Style** \n\n*Summary by Alexander Jung*\n\n* The paper describes a method to separate content and style from each other in an image.\n\n  * The style can then be transfered to a new image.\n\n  * Examples:\n\n    * Let a photograph look like a painting of van Gogh.\n\n    * Improve a dark beach photo by taking the style from a sunny beach photo.\n\n\n\n### How\n\n  * They use the pretrained 19-layer VGG net as their base network.\n\n  * They assume that two images are provided: One with the *content*, one with the desired *style*.\n\n  * They feed the content i... [[view more]](http://www.shortscience.org/paper?bibtexKey=journals/corr/1508.06576)", "score": 149, "likes": 0, "upvote": 149, "downvote": 0, "comments": [{"author": "ReginaldIII", "id": "dz6hym8", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hym8/", "content": "Good bot. Actually very good bot. ", "score": 108, "likes": 0, "upvote": 108, "downvote": 0, "comments": [{"author": "PrancingPeach", "id": "dz7npge", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7npge/", "content": "I briefly overlooked the top few lines and thought the bot actually generated the summary algorithmically, and I was like, \"damn, I want to see the paper on the bot now...\"", "score": 18, "likes": 0, "upvote": 18, "downvote": 0, "comments": 0}, {"author": "GoodBot_BadBot", "id": "dz6hyqa", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hyqa/", "content": "Thank you, ReginaldIII, for voting on shortscience\\_dot\\_org.  \n\nThis bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/).  \n\n ***  \n\n^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!", "score": 9, "likes": 0, "upvote": 9, "downvote": 0, "comments": [{"author": "poopypoopersonIII", "id": "dz6uils", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6uils/", "content": "Good bot", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "therealkainoa", "id": "dz75kd7", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz75kd7/", "content": "Bad person ", "score": 12, "likes": 0, "upvote": 12, "downvote": 0, "comments": [{"author": "poopypoopersonIII", "id": "dz76wkt", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz76wkt/", "content": "No u", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "agree-with-you", "id": "dz76wpv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz76wpv/", "content": "No you both", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "scionaura", "id": "dz7kipw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7kipw/", "content": "Me too thanks", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}]}]}]}]}]}, {"author": "daymanAAaah", "id": "dz6xw6u", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6xw6u/", "content": "I thought I was reading a person\u2019s comment until I saw yours. Damn that\u2019s good.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "ReginaldIII", "id": "dz700gs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz700gs/", "content": "You are reading a a person's comment. It's a summary aggregate tool which has shown us a user written paper summary that can be voted and commented on just like a reddit thread. \n\nI much prefer this style of summary bot to fully automated ones that generate the text as they tend skew the sentiment of the text and misplace emphasis.", "score": 9, "likes": 0, "upvote": 9, "downvote": 0, "comments": 0}]}, {"author": "progfu", "id": "dzxw32h", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzxw32h/", "content": "I wish all papers had 1-2 sentence summary that actually describes wtf is going on. I guess it should be the abstract, but so often I'm looking at something and just trying to figure out wtf.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "the1fundamental", "id": "e09ck5n", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e09ck5n/", "content": "good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "bhargav-patel", "id": "e0t8bu9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0t8bu9/", "content": "good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "bradfordmaster", "id": "dz72cex", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz72cex/", "content": "Good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "SmasimGirl", "id": "dz7dbn1", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7dbn1/", "content": "Good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "fucksfired", "id": "dz80f1i", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz80f1i/", "content": "Good bot", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "gwern", "id": "dz6pjzs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6pjzs/", "content": "I was thinking of style transfer too. It is a completely unexpected result from an approach which seems like it shouldn't do anything, and it produces results a layman can instantly understand and appreciate esthetically and which really do seem like 'it's thinking'. You couldn't ask for a better demonstration of the power and creativity of machine learning. (Although maybe OP is thinking more in a mathematical elegance vein by 'machine learning is beautiful'.)", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": [{"author": "ajmooch", "id": "dz6wwmr", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6wwmr/", "content": "I also like it from the point of view of the question,\n\"How far removed is what we consider 'true intelligence' from the simple mathematical tricks we employ to do ML today?\"\n\nIf you had never seen or heard of style transfer, and someone asked you \"On a scale of linreg to AGI, how difficult is it to paint a picture in the style of an artist? Of any artist?\"\n\nThe fact that this can be passably done with some gram matrix trickery and a simple optimizer, well, I find that beautiful.", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": [{"author": "techlos", "id": "dzr9es0", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzr9es0/", "content": "what i find even more fascinating is the fact that you don't even need gram matrices - different statistical measures give different aspects of the style.\n\nUsing the channel mean & std also conveys stylistic information, however it's the more generalized, patterned information. Using histogram matching is incredible for replicating small details. Essentially, the style can be summarized as being the overall distribution of feature map activations, while the content is captured within the feature maps themselves. Realizing this opened my mind up to so many more possibilities for machine learning in terms of artistic uses.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "ginsunuva", "id": "dz7gkfu", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7gkfu/", "content": "The Gram matrix was something done by some guy named Bela something in the 90s I believe. His paper was on the types of statistics of texture.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "ispeakdatruf", "id": "dzjnizf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzjnizf/", "content": "Are you thinking of Bruno Levy? http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.857.1858&rep=rep1&type=pdf\n", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "BrotaroKubro", "id": "dz7ij20", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7ij20/", "content": "The paper on the [NEAT](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf) algorithm is my favorite ML/NN paper, and one I've recommended for friends when this kinda thing comes up. There are so many cool articles out there though. ", "score": 22, "likes": 0, "upvote": 22, "downvote": 0, "comments": [{"author": "tasubo", "id": "dzksd6f", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzksd6f/", "content": "I find it prettty cool that NEAT started getting some proper coverage. I remember when I first presented NEAT in Uni around 10 years ago and since that time basically I've never seen any mainstream research that would reference this work until like last year. I considered that to be extremely strange as the paper and research itself was amazing.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "ReallyBoringPerson", "id": "dzdt66k", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzdt66k/", "content": "It's a pretty neat paper", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "mynameisvinn", "id": "dzb015z", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzb015z/", "content": "good choice! ken stanley is one of the more original and thoughtful researchers. ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "aidan_morgan", "id": "dzbs2h8", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzbs2h8/", "content": "Yes, Stanley really did some amazing work with NEAT", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "Eymrich", "id": "dzxxuyb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzxxuyb/", "content": "I only work on ML in my free time, started 2 years ago. NEAT was the first thing I tryed to implement, and talking with friends about it was always cool and interesting, even for outsiders.\nSo it's the same for me \ud83d\ude00", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "nishankatwork", "id": "e0ha1qr", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0ha1qr/", "content": "[PAPERS 101 - How An AI Learned To See In The Dark?](https://medium.com/click-bait/papers-101-how-an-ai-learned-to-see-in-the-dark-d05fa1d60632?source=linkShare-4dc3d3b2cdec-1528716404) ", "score": 20, "likes": 0, "upvote": 20, "downvote": 0, "comments": [{"author": "Sau001", "id": "e1rpqj2", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1rpqj2/", "content": "Wow! ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "deepfiz", "id": "e3avagm", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3avagm/", "content": "Wow! ", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "halffloat", "id": "dz71ikn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz71ikn/", "content": "\"Rapid Object Detection using a Boosted Cascade of Simple Features\"\n\nhttps://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf\n", "score": 18, "likes": 0, "upvote": 18, "downvote": 0, "comments": [{"author": "qiyu_cs", "id": "dzh56zm", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzh56zm/", "content": "Me too. Very nice paper and easy to follow. I really like the evaluation part of the paper.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}, {"author": "prashant-kikani", "id": "e1rrhct", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1rrhct/", "content": "Anything related to health is beautiful for everyone. Bioinformatics is convergence of Biology & ML.\n\nI will show how ML have helped doctors (ultimately humans) to save lives.\n\nAt the end of the day, beauty is, how technology helped to improve quality of human life & on how many faces ML brings smile because of its power of understanding data.", "score": 17, "likes": 0, "upvote": 17, "downvote": 0, "comments": [{"author": "galwayhooker", "id": "e35e4k5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e35e4k5/", "content": "hear hear", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "_mulcyber", "id": "e1yqjmq", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1yqjmq/", "content": "Do you have any example papers/sources?\n\nI would love to get into Bioinformatics but I didn't go much further than Medical Imaging.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "yetipirate", "id": "e24ybwb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e24ybwb/", "content": ">I would love to get into Bioinformatics but I didn't go much further than Medical Imaging.\n\nyou stopped before the fun part :) genomics especially is challenging and has lots of room for improvement. If you take a close look at any of the papers published in nature or w/e you will see there are some pretty big flaws and limiting factors. worth a look imo.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "prashant-kikani", "id": "e1zahkv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1zahkv/", "content": "Sure. Here are some links. \n\nOne of the latest material I found is this.\nOpportunities and obstacles for deep\r\nlearning in biology and medicine\nhttp://rsif.royalsocietypublishing.org/content/15/141/20170387\n\nDeep learning for computation Biology\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4965871/\n\nThese links can be found by just Google search..\nThere are tons of material out there to learn.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "_mulcyber", "id": "e1zuayx", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1zuayx/", "content": "Thanks! ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}, {"author": "michaeltrs", "id": "e3c7xn4", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3c7xn4/", "content": "Bahdanau's attention paper [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473). A simple modification enabling neural nets to focus selectively on different parts of the data, beautiful!", "score": 17, "likes": 0, "upvote": 17, "downvote": 0, "comments": [{"author": "shortscience_dot_org", "id": "e3c7y24", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3c7y24/", "content": "I am a bot! You linked to a paper that has a summary on ShortScience.org!\n\n**Neural Machine Translation by Jointly Learning to Align and Translate** \n\n*Summary by Abhishek Das*\n\nThis paper introduces an attention mechanism (soft memory access)\n\nfor the task of neural machine translation. Qualitative and quantitative\n\nresults show that not only does their model achieve state-of-the-art BLEU\n\nscores, it performs significantly well for long sentences which was a\n\ndrawback in earlier NMT works. Their motivation comes from the fact that\n\nencoding all information from an input sentence into a single fixed length\n\nvector and using that in the decoder was probably a bottleneck. Inste... [[view more]](http://www.shortscience.org/paper?bibtexKey=journals/corr/BahdanauCB14)", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "AbheekG", "id": "e43ldbc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e43ldbc/", "content": "Good bot!", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}]}]}, {"author": "fredtcaroli", "id": "e2ylyd5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ylyd5/", "content": "My vote goes out to Cycle GANs: [https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)  \n\n\nSuch a complex task \"solved\" with clever loss functions", "score": 14, "likes": 0, "upvote": 14, "downvote": 0, "comments": 0}, {"author": "2ros0", "id": "e3hl975", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3hl975/", "content": "the dropout layer! such a simple yet effective concept -- wish my brain had dropouts [http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)", "score": 15, "likes": 0, "upvote": 15, "downvote": 0, "comments": [{"author": "sigmoidp", "id": "e504ovw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e504ovw/", "content": "you can add dropout, it's called beer.\n\n&#x200B;", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "DonnyCrystal", "id": "e437w3q", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e437w3q/", "content": "Of course you do. Or you won't forget anything.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "None", "id": "e3j5hgp", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3j5hgp/", "content": "unused neurons tend to wither / growth occurs proportional to use.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "moewiewp", "id": "e4gc9iw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4gc9iw/", "content": "your brain got a pruning mechanism - which is like a permanent dropout mechanism with drop rate really really close to 1", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "serge_cell", "id": "e4i6vho", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4i6vho/", "content": "AlphaGo Zero", "score": 15, "likes": 0, "upvote": 15, "downvote": 0, "comments": 0}, {"author": "mutalk", "id": "e1jzyhf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1jzyhf/", "content": "I found 'Differentiable Neural Computers' by Deepmind fascinating. \nhttps://deepmind.com/blog/differentiable-neural-computers/", "score": 15, "likes": 0, "upvote": 15, "downvote": 0, "comments": 0}, {"author": "melliKR", "id": "e3g0wep", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3g0wep/", "content": "The Wasserstein Auto-Encoder (WAEs) paper: [https://arxiv.org/abs/1711.01558](https://arxiv.org/abs/1711.01558). Super elegant generalization of + first theoretical justification for Adversarial Auto-Encoders (AAEs). ", "score": 14, "likes": 0, "upvote": 14, "downvote": 0, "comments": [{"author": "qwertypi123", "id": "e3pzrp3", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3pzrp3/", "content": "Liked how this one tied together AAEs with a solid reason for their good performance", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "XalosXandrez", "id": "dz6gthn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6gthn/", "content": "This reinvention of variational inference by Geoff Hinton is a delight to read \\- [Keeping the neural networks simple by minimizing the description length of the weights](http://svn.ucc.asn.au:8080/oxinabox/Uni%20Notes/honours/Background%20Reading/hinton1993keeping.pdf) .\n\nI'm a big fan of the \"bits\\-back\" argument and the whole communication theory viewpoint \\(minimum description length\\) of regularization.", "score": 27, "likes": 0, "upvote": 27, "downvote": 0, "comments": [{"author": "None", "id": "dz6hj6a", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6hj6a/", "content": "[deleted]", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "yetipirate", "id": "dz6ky1p", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6ky1p/", "content": "It's rly popular! (Lasso anyone?) It's also undecidable to find an algorithm that solves for the minimum description length.  ", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": [{"author": "yldedly", "id": "e16faet", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e16faet/", "content": "> It's also undecidable to find an algorithm that solves for the minimum description length. \n\nThat's very interesting, do you have a link?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "yetipirate", "id": "e18ivuc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e18ivuc/", "content": "no link on hand, look up kolmogorov complexity.\n\nThe idea is simple, if you have some algorithm *A(l)* that can find the minimum length descriptor of some language, *l* then you can devise a language *B* whose MDL is of length |*A*| + c, where c is some constant. You then could describe B using *A(B)* thus achieving a description shorter than *B*'s MDL. A contradiction.\n\nSomething like that.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}]}, {"author": "question99", "id": "e06alpf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e06alpf/", "content": "I finished reading the paper now but I only understood around 50% of it. The parts where they are invoking statistical mechanics are increasingly challenging (maybe just for me as I have very little experience with the topic).", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "sieisteinmodel", "id": "e1m2nns", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1m2nns/", "content": "Why reinvention? Which other paper would be the invention paper that predates this one?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "piconzaz", "id": "dzthnmy", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzthnmy/", "content": "ML is a vast field. I'll let you choose the \"sub-field\" you're interested in and will rather focus on some authors who produce beautiful research and write their papers very well. When you read those guys, you definitely ARE smarter after.\n\nLeon Bottou: Amazing use of sophisticated mathematical tools in a very gentle way. ([The Trade-offs of Large Scale learning](https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf) probably still is my favorite paper. [Wasserstein GAN](https://arxiv.org/abs/1701.07875) is pretty amazing too)\n\nSanjeev Arora: To me, he is \"the guy\" that has been pushing the limits of the theoretical understanding of ML and DL, in the recent years, with tools that AFAIK were not known in the community.\n\nMax Welling: I used to have a low tolerance for the Bayesian church, but always had a soft spot for this guy. Amazing work on Variational Inference and relating it to stochastic optimization.", "score": 14, "likes": 0, "upvote": 14, "downvote": 0, "comments": [{"author": "shortscience_dot_org", "id": "dzthnz1", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzthnz1/", "content": "I am a bot! You linked to a paper that has a summary on ShortScience.org!\n\n**Wasserstein GAN** \n\n*Summary by MarvMind*\n\nThis very new paper, is currently receiving quite a bit of attention by the [community]().\n\n\n\nThe paper describes a new training approach, which solves the two major practical problems with current GAN training:\n\n\n\n1) The training process comes with a meaningful loss. This can be used as a (soft) performance metric and will help debugging, tune parameters and so on.\n\n\n\n2) The training process does not suffer from all the instability problems. In particular the paper reduces mode collapse significantly... [[view more]](http://www.shortscience.org/paper?bibtexKey=journals/corr/1701.07875)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "ipoppo", "id": "e19oiz4", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e19oiz4/", "content": "good bot", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "namescost", "id": "e1xn6ww", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1xn6ww/", "content": "A blank piece of paper, with a crayon, of course.\n\n(assuming they're equipped to understand it)", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": 0}, {"author": "Devenar", "id": "e1kzcg5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1kzcg5/", "content": "[World Models](https://worldmodels.github.io/) (Here's [the paper](https://arxiv.org/abs/1803.10122)). They don't even have to have any knowledge about machine learning and it looks amazing.\n\nThe computer comes up with its own representation of games and then learns to play inside it's own \"dreams\" (to use the hype term)? That's beauty.", "score": 25, "likes": 0, "upvote": 25, "downvote": 0, "comments": [{"author": "thebackpropaganda", "id": "e2ayxfj", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ayxfj/", "content": "I'm curious to know what your background with ML is.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "Devenar", "id": "e2azefb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2azefb/", "content": "Just getting started. A couple years ago I learned the math behind feedforward neural networks, but struggled to use Tensorflow. Now that I\u2019ve found Keras, I\u2019m starting to put some ideas into practice to see what happens.\n\nIn short, I haven\u2019t read many papers, and when I read them, I usually don\u2019t understand all of the beauty of them. So this paper was special because the author built a website and you could actually understand what was going on, which I love.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "thebackpropaganda", "id": "e2b0y7r", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2b0y7r/", "content": "Thanks. So you liked the beauty of the presentation, not necessarily of the idea itself. That's fair.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "Devenar", "id": "e2b18ki", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2b18ki/", "content": "I\u2019d say I liked both. I think it\u2019s beautiful that a computer can come up with a simulation of how a world works and then learn to do things in the world based off of that simulation; I think it mimics the way humans plan a lot better than many of the other models, and I\u2019m excited for it to be applied to other more complex videogames so that APIs aren\u2019t needed.\n\nBut I think I appreciate the beauty of the presentation as well, because it allows me to actually see what the computer is seeing and how it plays games with itself (like making the balls disappear into thin air by making particular moves that trick the simulator) and learns.\n\nWhile the paper is nice and has a cool idea, I think there are many papers with lots of cool ideas that I don\u2019t have the time right now to understand. This one was great because I feel like anyone can understand its beauty. That part is due mostly to the website.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}]}]}, {"author": "pandeykartikey", "id": "e1ve895", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1ve895/", "content": "I would show him/her something like Neural Style Transfer ([https://arxiv.org/pdf/1508.06576.pdf](https://arxiv.org/pdf/1508.06576.pdf)).", "score": 12, "likes": 0, "upvote": 12, "downvote": 0, "comments": [{"author": "Maciekism", "id": "e21vacn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e21vacn/", "content": "Why are you assuming it's a him? ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "pandeykartikey", "id": "e21vw9x", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e21vw9x/", "content": ">Why are you assuming it's a him?\n\nMy Bad", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "Maciekism", "id": "e2dy0cz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2dy0cz/", "content": "Wow I got so much hate for pointing this out. I wasnt even serious... I guess people really hate when you point out subconscious biases. ", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": [{"author": "PM_ME_UR_HPARAMS", "id": "e2f784p", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2f784p/", "content": "Technically \"he/him\" is gender agnostic when the context is not clear (a practice which predates the founding of the United States): https://en.wikipedia.org/wiki/Singular_they#Prescription_of_generic_he\n\nThen some people had to turn it into a social justice issue.\n\n/u/pandeykartikey", "score": 18, "likes": 0, "upvote": 18, "downvote": 0, "comments": [{"author": "Maciekism", "id": "e2h9zhv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2h9zhv/", "content": "Not that i think its such a big deal in this example but the langue we speak does heavily influence our perception of the world. I am a bit allergic to certain social justice issues and people do take it too far all of the time (history rather then herstory for example) But in this case he/him became the common pronouns because of male dominance in the old world. Also people dont use he because they are aware of it being gender-less people use it because they are simply assuming a gender when having a thought. (at least majority of the time and as it was in this example proven by what he wrote down below) This isnt a tragedy but it does point to the fact that people do have unwarranted subconscious bias", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "PM_ME_UR_HPARAMS", "id": "e2hfwvw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2hfwvw/", "content": "Actually, the discussion of \u201che or she\u201d was brought up when the first grammar books were written. They decided against it because in several contexts the addition of the 2 words changed sentence structure and was clumsy. (One of those first grammar books was written by a female, by the way.)\n\nYour claim that there are more than 2 genders (one that I don\u2019t necessarily agree with but I\u2019ll play along) only serves to reinforce my point.\n\nShould we be writing our sentences as \u201che or she or xe or they or heshe...\u201d as each gender is invented? Am I \u201csexist\u201d if I don\u2019t include however genders there God-knows-how-many are?", "score": 9, "likes": 0, "upvote": 9, "downvote": 0, "comments": [{"author": "Maciekism", "id": "e2huo5v", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2huo5v/", "content": "I never claimed that there are more then 2 genders. I simply stated that people that use he/him dont usually use it because they are aware its gender neutral pronoun. Which actually in some cases it isnt so it makes things confusing and makes you really heavily on context. \n\nI also didnt call anyone a sexist. Having a subconscious bias doesnt necessarily mean youre a sexist. if it would then everyone would be sexist because everyone has a subconscious bias one way or another. In my opinion i wouldnt call someone a sexist unless their belief is a conscious one. If they actually think females are inferior then I would use the word sexist to describe them. Additionally being a female does not exclude you from being sexist against other females- Or holding believes that sexism is not harmful.\n\nI do not think it makes grammar awkward. Just use them/they/their", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": [{"author": "PM_ME_UR_HPARAMS", "id": "e2ivho8", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ivho8/", "content": "> them/they/their\n\nWhich are all plural pronouns and imply multiple individuals when it is not necessarily the case. This would be incorrect, grammatically. You'd probably miss points on the SAT for it.\n\nEven people who share your beliefs about gender advocate for \"he or she\".", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}]}]}]}]}]}]}]}, {"author": "gohu_cd", "id": "dz6ds5q", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6ds5q/", "content": "It's not a paper but the kernel trick for SVM is what got me into ML, because I thought it was a very elegant and clever trick.", "score": 31, "likes": 0, "upvote": 31, "downvote": 0, "comments": [{"author": "willis77", "id": "dz6p7ev", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6p7ev/", "content": "http://oneweirdkerneltrick.com/", "score": 15, "likes": 0, "upvote": 15, "downvote": 0, "comments": 0}, {"author": "hlynurd", "id": "dz6enoo", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6enoo/", "content": "[http://www.eric\\-kim.net/eric\\-kim\\-net/posts/1/kernel\\_trick\\_blog\\_ekim\\_12\\_20\\_2017.pdf](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick_blog_ekim_12_20_2017.pdf)", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": 0}, {"author": "claytonkb", "id": "dzzzgi5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzzzgi5/", "content": "For me, it was the hashing trick (similar motivation to probabilistic data structures like Bloom filters) -- here's [a paper](https://arxiv.org/abs/1504.04788) that takes the hashing trick to the next level.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "ApertureCombine", "id": "dz7l6jt", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7l6jt/", "content": "Granted I haven't read that many papers, but I find [this](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) paper on word2vec really cool. Being able to extract various complex relationships between words just from a general corpus of text is incredibly interesting.", "score": 11, "likes": 0, "upvote": 11, "downvote": 0, "comments": [{"author": "ai_math", "id": "e1908ff", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1908ff/", "content": "[This paper](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf) which connects the skip-gram negative sampling model if word2vec to factorization of PMI matrices is one of my favorites.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "holt0102", "id": "e3bo7ad", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3bo7ad/", "content": "Something about AutoEncoders. Always found the idea fascinating.", "score": 11, "likes": 0, "upvote": 11, "downvote": 0, "comments": [{"author": "gunthercult28", "id": "e3dinol", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3dinol/", "content": "I'm excited by the prospect of using autoencoders for imputing features, effectively using them to clean dirty databases.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "holt0102", "id": "e3drl7i", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3drl7i/", "content": "Could you elaborate on that ? It sounds interesting. \n\nLike automatically cleaning databases training the \u201cCleaner\u201d with good normal entries of data, to fix the bad ones ?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "gunthercult28", "id": "e3emsry", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3emsry/", "content": "100.\n\nThe autoencoder acts to normalize the data down to smaller feature space. It's not guaranteed to perfectly return the resulting image of grandma you input, but rather subtle differences should come out.\n\nIn this case, we want to leverage those subtle differences to generally impute values based on the topology of our feature space. So rather than declare an imputer per feature, we declare a network that encodes what invoice data topology at Company XYZ should look like in our database and fixes errors.\n\nThis is just me imagining. I don't have the practical knowledge to build and train this kind of network, but theoretically it can learn to encode arbitrary numerical spaces to a subspace, and further it acts as a function transforming an input object to almost itself as an output object through that subspace transformation.\n\nAllowing for corrections is essentially a threshold for acceptible error between input and output data.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}]}]}]}, {"author": "evc123", "id": "dz6gwvd", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6gwvd/", "content": "Translating Neuralese\n\nhttps://arxiv.org/abs/1704.06960", "score": 29, "likes": 0, "upvote": 29, "downvote": 0, "comments": [{"author": "twocatsarewhite", "id": "dz6jxjt", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6jxjt/", "content": "Anything by that guy really? Speaking of which, how the heck does he regularly publish 3+ high quality work a year?", "score": 17, "likes": 0, "upvote": 17, "downvote": 0, "comments": [{"author": "sensual_onlooker", "id": "dz6p396", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6p396/", "content": "May I know whom you are referring? It's a 3 author paper. Im into AI now. I need a strong base to compound my knowledge. I thought this sub consensus is enough to pick a virtual mentor.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "MTGTraner", "id": "dz6p90e", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6p90e/", "content": "The first author is the default reference.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "sensual_onlooker", "id": "dz6pook", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6pook/", "content": "Thanks", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "twocatsarewhite", "id": "dz7mpo3", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7mpo3/", "content": "Yeah, Jacob Andreas is who I meant as well.\n\nEdit: Typo", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}, {"author": "clifgray", "id": "e1q56y2", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1q56y2/", "content": "This sounds eerily like what a robot trying to learn more about machine learning would say.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "sensual_onlooker", "id": "e1q6d6w", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1q6d6w/", "content": "Thanks human\n\n^^beep ^boop ^Iam ^^a ^^bot\n\n", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}, {"author": "terrorlucid", "id": "e06o56v", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e06o56v/", "content": "how many years has he done that?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "rlrgr", "id": "dz6szsc", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6szsc/", "content": ">To obtain human annotations for this task, we recorded both actions and messages gener- ated by pairs of human Amazon Mechanical Turk workers playing the driving game with each other. We collected close to 400 games, with a total of more than 2000 messages exchanged, from which we held out 100 game traces as a test set.\n\nSomething about this just blows my mind. The human race is farming itself for intelligent behavior... with the goal of improving (or perhaps replacing) itself.", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": [{"author": "phobrain", "id": "dz7gql5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7gql5/", "content": "I'm mining myself for a sort of unintelligent behavior (think Rorschach test), to create a sort of self-awareness training to help humanity stay on top of things. You're welcome! :-)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "proimprobable", "id": "e26mwbm", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e26mwbm/", "content": "Surprised all top answers are for very recent papers. Anyway, I personally find fascinating the connection of physics to ML, and Neal's paper on HMC is such an intuitive and clever paper in that category.", "score": 9, "likes": 0, "upvote": 9, "downvote": 0, "comments": 0}, {"author": "nachiket273", "id": "dz6yw8b", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6yw8b/", "content": "Original paper of [GAN](https://arxiv.org/pdf/1406.2661.pdf) . ", "score": 17, "likes": 0, "upvote": 17, "downvote": 0, "comments": 0}, {"author": "jamsawamsa", "id": "e2nks25", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2nks25/", "content": "Mind would have to be [Imagination Augmented Agents for Deep RL](https://nurture.ai/papers/imagination-augmented-agents-for-deep-reinforcement-learning).\n\nI love how some of the most novel methods that people have come up with to train RL agents end up being so similar to how the human brain learns as well. And to bring in \"imagination\" into it's training. \n\nJust love the concept.", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": [{"author": "gagablob", "id": "e3t4ukf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3t4ukf/", "content": "Do we know how the human brain learns?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "jamsawamsa", "id": "e3t5l62", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3t5l62/", "content": "Neuroscience and behavioral psychology studies mate. I think the first chapter of the Barto n Sutton book gives a good intro to how reinforcement learning came about ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "gagablob", "id": "e3t6gjv", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3t6gjv/", "content": "The intricacies of brain are still unknown and neuroscientist are yet to understand all about how human brain learns. And isn't reinforcement learning engineered just based on the concept of pavlovian reinforcement learning?", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "jamsawamsa", "id": "e3t7qjs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3t7qjs/", "content": "Yeah, you are right. RL models the concept of pavlovian RL. Guess it's my wording I got to work on in the original statement.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}]}]}, {"author": "Tsadkiel", "id": "e3v9v63", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3v9v63/", "content": "Stacked GANs.  Put in a text description.  Get a birb", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}, {"author": "stgstg27", "id": "e449547", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e449547/", "content": "Paper on GAN. The idea that adversarial networks leading to better generation is beautiful in itself \n\n", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "nobodykid23", "id": "e481reo", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e481reo/", "content": "Don't forget the following GAN tutorial. It's a good intro for someone who want to start diving into GANs", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "deluded_soul", "id": "e4cejah", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4cejah/", "content": "Can you share the link? ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "nobodykid23", "id": "e4cvgyj", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4cvgyj/", "content": "Here you go : [NIPS 2016 tutorial: Generative adversarial networks](https://arxiv.org/abs/1701.00160)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}]}, {"author": "shayanfazeli", "id": "e0i2ao1", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0i2ao1/", "content": "I would choose the paper \"Show, Attend, and Tell\".", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}, {"author": "jayjay59", "id": "e4fxguk", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4fxguk/", "content": "Link to a freemind mind map with links to all papers referenced in this article can be found  here https://imgur.com/a/UHde66q", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": [{"author": "imguralbumbot", "id": "e4fxh54", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4fxh54/", "content": "^(Hi, I'm a bot for linking direct images of albums with only 1 image)\n\n**https://i.imgur.com/73GUiYF.jpg**\n\n^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&subject=ignoreme&message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&subject=delet%20this&message=delet%20this%20e4fxh54) ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}, {"author": "min_sang", "id": "dz6ni5x", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6ni5x/", "content": "[Wavenet](https://arxiv.org/pdf/1609.03499.pdf) ", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": [{"author": "geoffreygoines", "id": "e08omkp", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e08omkp/", "content": "[Parallel WaveNet](https://arxiv.org/pdf/1711.10433.pdf) is amazing too", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}]}, {"author": "DeepDreamNet", "id": "e1g7kzx", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1g7kzx/", "content": "I'd actually pick a paper that shows the strength of the community, specifically the Google technical debt paper - they worked hard to make it accessible, they know this stuff cause they operate at huge scale, and they put out a map for friend and foe alike to keep us from falling in the same potholes they did -- [https://ai.google/research/pubs/pub43146](https://ai.google/research/pubs/pub43146)", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}, {"author": "3pence", "id": "e369bfx", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e369bfx/", "content": "Rosenblatt's perceptron.\nIt is one of the most tangible and beautiful mathematics doing wonders examples there is. Linear Bound Domains aside.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "TakoTabak", "id": "dzk3p9f", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzk3p9f/", "content": "This one\\-shot paper [https://arxiv.org/abs/1702.06559](https://arxiv.org/abs/1702.06559) started for me the idea of combining reinforment learning and superviced learning. GAN also does this but still I like the approach taken her.", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": [{"author": "tomvorlostriddle", "id": "dzrjkcf", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzrjkcf/", "content": ">superviced learning\n\nSounds dangerous", "score": 11, "likes": 0, "upvote": 11, "downvote": 0, "comments": 0}]}, {"author": "diggerdu", "id": "dzxkv2h", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzxkv2h/", "content": "One big net for everything", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "CindicatorPusheens", "id": "e4njjn0", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4njjn0/", "content": "I would actually start some explanatory articles with examples and codes to people who don't have a lot of knowledge, but would understand ML better and may become interested in the field more\n\nLike this one [Music generation with Neural Networks\u200a\u2014\u200aGAN of the\u00a0week](https://medium.com/cindicator/music-generation-with-neural-networks-gan-of-the-week-b66d01e28200)", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}, {"author": "MysteriousBus5", "id": "e2cu7hp", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2cu7hp/", "content": "How one needs only two variables/features to know what the number is. This is pure magic.\n\n[https://i.stack.imgur.com/2gSs1.png](https://i.stack.imgur.com/2gSs1.png) ", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "SomeRandomGuydotdot", "id": "e2l7c60", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2l7c60/", "content": "I'm partial to a slightly different school.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)\n\nThe original paper regarding residual networks, and hella interesting if viewed from the pure math perspective, as they should be equivalent, but in practice are not. Really drives home that universal approximation is not the cure all.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n[http://cnnlocalization.csail.mit.edu/supp.pdf](http://cnnlocalization.csail.mit.edu/supp.pdf)\n\nProbably the most clear cut way of describing GAP-CNNs attention regions I've ever seen. CAMs changed how I understood features.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nAs for the paper I care about most in my work (Fuck it, couldn't find it but this is close):\n\n[https://www.ijcai.org/Proceedings/15/Papers/561.pdf](https://www.ijcai.org/Proceedings/15/Papers/561.pdf)\n\nThe key takeaway, is that CNNs can be applied to time series data, and out perform bag of features crafted by PhDs.\n\nThis implies to me, the general applicability of CNNs to many real world problems.\n\nThis is beautiful because it simplifies a whole class of hard problems, into simple coding and data collection. A more approachable work flow compared to running a major research department.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "shortscience_dot_org", "id": "e2l7cv0", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2l7cv0/", "content": "I am a bot! You linked to a paper that has a summary on ShortScience.org!\n\n**Deep Residual Learning for Image Recognition** \n\n*Summary by Martin Thoma*\n\nDeeper networks should never have a higher **training** error than smaller ones. In the worst case, the layers should \"simply\" learn identities. It seems as this is not so easy with conventional networks, as they get much worse with more layers. So the idea is to add identity functions which skip some layers. The network only has to learn the **residuals**. \n\n\n\nAdvantages:\n\n\n\n* Learning the identity becomes learning 0 which is simpler\n\n* Loss in information flow in the forward pass is not a problem a... [[view more]](http://www.shortscience.org/paper?bibtexKey=journals/corr/HeZRS15)", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "benevolentpirate", "id": "e2eocep", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2eocep/", "content": "Can you please explain? To-be-junior undergrad here. :)", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "MysteriousBus5", "id": "e2euxoo", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2euxoo/", "content": "Sure :) \n\nThe graphs are related to the dimensionality reduction. The experiment is try to reduce the dimensionality of MNIST dataset (set of images of hand-drawn digits) as much as possible without loosing their separability in the lower dimensional space. \n\nOn the right is algorithm PCA which reduces the dimension by eliminating the directions which have less variance and then projecting the data on the remaining dimensions. On the left is auto-encoder (think it like a neural network same number of nodes at input and output layers, but very few at the middle, 2 in this case) which feeds the image at input layer and expect the same image at output layer, but near the middle of the network, the number of layers are drastically reduced, thus creating a squeezing kind of process, or information bottleneck kind of phenomenon.\n\nThe magic is in the output. Consider the right image, all the colours are distributed across the 2-D space with no or very less overlap. Feel this as, you are like a magician, who is allowed to ask only two questions about the image, and based on answer, you'll be able to very well predict the number. Just two questions, or two features was enough to know the entire number.\n\nStill, didn't get the magic ?", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": [{"author": "Ciductive", "id": "e2fm2oj", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2fm2oj/", "content": "Shouldn't you theoretically be able to reduce it to only one dimension? The number itself?\n\nIf I'm only allowed to ask one question and I ask what the label/number is, isn't that clearly sufficient to know the number? ", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "None", "id": "e2gxjvd", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2gxjvd/", "content": "> Shouldn't you theoretically be able to reduce it to only one dimension? The number itself?\n\nTheoretically yes. Theoretically, you could have a projection that projects the data onto a real or natural number line. And with the right scaling, the points belonging to the 0 fall on 0, 1 on 1, etc.\n", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "Ciductive", "id": "e2hzba6", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2hzba6/", "content": "So why is this impressive then?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "MTGTraner", "id": "e2i0ww3", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2i0ww3/", "content": "Presumably because this is done in an unsupervised manner.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "Ciductive", "id": "e2i25pz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2i25pz/", "content": "What a stupid neural net haha, what kind of idiot needs two numbers to represent a single number haha", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": [{"author": "archxeon", "id": "e2i5as8", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2i5as8/", "content": "You are actually representing  28 x 28 numbers by two numbers.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "Ciductive", "id": "e2i5yxa", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2i5yxa/", "content": "When you should be able to do it with one...", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": []}]}]}]}, {"author": "None", "id": "e2ichwp", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ichwp/", "content": "well, I think you are fixated too much on the \"number\" as label. Instead of images of digits, just think of it as arbitrary objects, like house, ball, table, chair, etc. and then you maybe get a better feeling why grouping images via only two numbers is impressive", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}, {"author": "PM_ME_UR_HPARAMS", "id": "e2f6yae", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2f6yae/", "content": "> which reduces the dimension\n\nCould you not use SVD and select better principal components to accomplish the same thing as the autoencoder?\n\nAssuming you can use a hyperplane to divide each category.\n\nEDIT: Nvm I'm dumb I just realized that the magic is the autoencoder selected those very \"better principal components\" I was talking about.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "benevolentpirate", "id": "e2ezvbh", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2ezvbh/", "content": "Wow. This is really cool!\nThanks for explaining! :)", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "PhysLane", "id": "e33ck8p", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e33ck8p/", "content": "I am curious if you can go into more detail about how the auto-encoder works.  Does it require normalization? (\\*)\n\n\\*If so then would it not be susceptible to the same issues PCA has with using Z-scores for normalization?  Or are they worse because of scaling of normalization affects computation time.  Conjugate Gradient Descients performance (computation time) is affected by normalization (called parameter scaling), so since so many networks are based on Gradient Descent, I wondered how much normalization affects the reduction.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}, {"author": "proptrot13", "id": "e2uvoea", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2uvoea/", "content": "So then you feed this data through a neural network and it must improve performance by a lot?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "alkalait", "id": "e3wb209", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3wb209/", "content": "I don't agree. Sure you've learned a mapping from (mnist) image space to 2D, but you still to retain the all the information that defines the mapping.\n\nSame deal with PCA: you need the principal components (aka loadings), in addition to the reduced-dimension coordinates (Score 1 & 2 in the figure), to fully describe a number's mnist image.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "randiscML", "id": "e4qm1yw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4qm1yw/", "content": "The original EXP3 paper. It is written so beautifully, I think it is fascinating what you can do with bandit feedback.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}, {"author": "mbox171", "id": "dzhwx87", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzhwx87/", "content": "YOLO", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "vannak139", "id": "dzm87g4", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzm87g4/", "content": "The Yolo v3 paper is, like v1 and v2, a gem. ", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}]}, {"author": "chrispher2012", "id": "e0h8ja2", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0h8ja2/", "content": "T-SNE, show me how to analysis the machine learning problem!", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "roeiherzig", "id": "e0haqgb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0haqgb/", "content": "My personal choice is \"[Relations Networks](https://arxiv.org/pdf/1706.01427.pdf)\" from DeepMind, which made a lot of impact on my recent work on Graph Networks for capturing Permutation Invariance on [Scene Graphs prediction](https://arxiv.org/pdf/1802.05451.pdf) (submitted to NIPS18 ).\n\nWhat so special on the DeepMind paper is that I think it tries to bridge the gap between DL and classic ML (probability graphical model). I really love that.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "geomtry", "id": "e1c0k3b", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1c0k3b/", "content": "Indeed this is a very exciting direction", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "russel_russel", "id": "e3vvufq", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3vvufq/", "content": "My vote goes to Style Transfer for Decorated Logo Generation: https://www.groundai.com/project/contained-neural-style-transfer-for-decorated-logo-generation/ ", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}, {"author": "rosivagyok", "id": "e4cri9t", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e4cri9t/", "content": "Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural Networks\n\nhttps://arxiv.org/abs/1606.04393", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "zhumao", "id": "dz6icav", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6icav/", "content": "Cybenko's [original paper](https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf) on the universal approximation property of superpositions of sigmoidal functions which:\n\n1. established the theoretical foundation of NN, moreover\n\n2. NN with one hidden layer neurons suffices \n\nSince then, works on NN are just tinkering, engineering, and hype e.g. deep learning.", "score": 15, "likes": 0, "upvote": 15, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz6xo9y", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6xo9y/", "content": "I'm a bit of a broken record on this topic, but polynomials satisfy the same property (see the Stone-Weierstrass theorem), so I guess by the same logic you could say that everything that's been done in ML or proto-ML for at least the last 70+ (maybe even 130) years is just 'tinkering, engineering, and hype.' \n\nThat's obviously absurd.", "score": 32, "likes": 0, "upvote": 32, "downvote": 0, "comments": [{"author": "zhumao", "id": "dz6zmbt", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6zmbt/", "content": ">That's obviously absurd.\n\nThe superposition/composition of sigmoidal function approximation only requires a single hidden layer neurons i.e. one composition, unlike polynomials used in S-W thoerem, no unknown higher power needed, nor restricted to compact domains.\n\nThe comparison is absurd.\n\nAs a final note, this result also \"rescued\" NN from Marvin Minsky's famous counter example on perceptron's inability to solved the XOR problem.", "score": -5, "likes": 0, "upvote": -5, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz70y7j", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz70y7j/", "content": "Instead of an unknown number of terms needed in your polynomial basis you need an unknown number of neurons in that single layer, and, yes, there is indeed a restriction to a compact domain (the unit hypercube) in Cybenko's paper.\n\nWhat I was calling absurd is the claim that everything since is just 'tinkering, engineering, and hype.' Cybenko's theorem has little bearing on whether neural networks are useful in practice. ", "score": 30, "likes": 0, "upvote": 30, "downvote": 0, "comments": [{"author": "zhumao", "id": "dz7200s", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7200s/", "content": ">Instead of an unknown number of terms needed in your polynomial basis you need an unknown number of neurons\n\nThat's where engineering comes in, and in comparison how much \"engineering\" effort is done since S-W theorem was proved, hence how useful was that in comparison? ", "score": -1, "likes": 0, "upvote": -1, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz72cvw", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz72cvw/", "content": "I don't understand the question. ", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "zhumao", "id": "dz72pin", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz72pin/", "content": "How useful is S-W theorem in comparison, for example in ML i.e. was polynomials being seriously considered as an important tool?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz736lr", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz736lr/", "content": "I guess neither polynomial regression nor single layer neural networks are that important as practical tools in ML. \n\nIf, however, all that mattered were results about function classes being dense in the set of continuous functions restricted to compact domains, then they'd be equally important. But that's not all that matters. Neither S-W nor Cybenko's theorem say anything about learning from data, which is kind of critical in ML. \n\nAs a side note, there's a lot of theory related to kernel methods (of which polynomial regression is one instance) which is more pertinent to actually learning from data than S-W. ", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "zhumao", "id": "dz7834i", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7834i/", "content": "> then they'd be equally important. But that's not all that matters. \n\nNo, it's not equivalent, the body of work on NN dominates.\n\n>Neither S-W nor Cybenko's theorem say anything about learning from data, which is kind of critical in ML.\n\nBoth approximation theorems clearly proved otherwise as part of ML, except one is more useful than the other, so far.", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": [{"author": "clurdron", "id": "dz78pmz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz78pmz/", "content": "You seem very confident, at least.", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}]}]}]}]}]}]}]}]}, {"author": "GVR64", "id": "dzg85r9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzg85r9/", "content": "The proof of Cybenko is non constructive and uses Hahn -Banach beautifully. I do not know of any constructive proof using the Hahn -Banach to construct an approximation as formulated in Cybenko paper. It is a beautiful paper but to call other NN works as tinkering and hype is very harsh and unfair. You can justify your stance a bit, if you work in the field of Convexity and Optimisation and feel neglected about the relative lack of spotlight on your area compared to Deep learning. With waning public funding and interest the only way we can fund and pursue Hahn Banach like gems is through generating hype and memes like these. If everything is only about proving existence elegantly then we may as well ask \"Who is this Magellan person anyway?\" ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "zhumao", "id": "dzgdmr9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzgdmr9/", "content": ">The proof of Cybenko is non constructive and uses Hahn -Banach beautifully. I do not know of any constructive proof using the Hahn -Banach to construct an approximation as formulated in Cybenko paper.\n\nIndeed, the proof is dope but not sure the non-constructive argument is a blemish. Non-constructive proofs, and many are beautiful, is rife in math, and not just in analysis e.g. the fundamental theorem of algebra, the number of primes is infinite, etc.\n\n>It is a beautiful paper but to call other NN works as tinkering and hype is very harsh and unfair. You can justify your stance a bit,....\n\nThe result also provided an explicit expression (though not fixed in number of hidden layer neurons) to formulate the problem as well as rescued NN from Marvin Minsky's famous/infamous example of the limitation of [NN/perceptrons](https://en.wikipedia.org/wiki/Perceptrons_(book). \n\nOTOH, do plead guilty on the tone, I should've added \"applications\". As for \"hype\", there are plenty, and here is a a typical [example](https://arxiv.org/abs/1608.08225) (which I find the level of hype and the vacuousness in content has reached to a new height), from the *New Physics* crowd no less. \n\n>you can justify your stance a bit, if you work in the field of Convexity and Optimisation and feel neglected about the relative lack of spotlight on your area compared to Deep learning. With waning public funding and interest the only way we can fund and pursue Hahn Banach like gems is through generating hype and memes like these. If everything is only about proving existence elegantly then we may as well ask \"Who is this Magellan person anyway?\"\n\nPublic funding is not an issue here since not in academia, I work in applications of optimization, and mostly non-convex problems and NP-hard problems which included using NN as a tool.\n", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "cognizant_ape", "id": "dzi149p", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzi149p/", "content": "I like to point out that the proof was not only non\\-constructive but it didn't establish the learnability of such functions, just their existence.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "p_pistol", "id": "dz6rfrn", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6rfrn/", "content": "Yup, that's what I was thinking too.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "sweatyCameltoe", "id": "dz7kqzs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7kqzs/", "content": "Nvidia's end to end learning for self driving cars https://arxiv.org/abs/1604.07316\n\nIt has some feature visualisations so you can \"see\" something and there are nice videos of it working. With only a 100hours of driving data.. This was my eye opener", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "deck13", "id": "dze6nxb", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dze6nxb/", "content": "You may have to read between the lines:\n\nhttp://www.ams.org/journals/bull/2018-55-01/S0273-0979-2017-01597-2/home.html", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "cucfufofo", "id": "dzp8yiy", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzp8yiy/", "content": "A Unifying Review of Linear Gaussian Models", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "guugvidil", "id": "dzp9lvz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzp9lvz/", "content": "A Unifying Review of Linear Gaussian Models", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "udnaan", "id": "e28oaov", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e28oaov/", "content": "You can't. Unless if they are already heavily invested into programming or mathematics. You can however show them the by products and application of machine learning.\n\nPick your favorite but I usually go with VAEs (such as my profile image)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "_10ZIN_", "id": "e552kh9", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e552kh9/", "content": "**Text to image synthesis using GAN**\n\nThe title of the paper itself is so riveting, it for sure calls upon an exhilarating  read.\n\nshout out to Ian Goodfellow for inventing GAN, to make deep learning all the more special.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "battboe", "id": "dz6wry5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6wry5/", "content": "Old school vision paper that got started.. [Modeling the shape of the scene: a holistic representation of the spatial envelope](http://people.csail.mit.edu/torralba/code/spatialenvelope/)", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "jem-mosig", "id": "dz7etaz", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz7etaz/", "content": "I love this one, making a fascinating link to physics: \"Why does deep and cheap learning work so well?\" https://arxiv.org/abs/1608.08225", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "efavdb", "id": "e0qbtev", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e0qbtev/", "content": ">A Unifying Review of Linear Gaussian Models\n\nAgree, it includes a lot of interesting insight that makes dl seem less magical to me.  Whether the ideas there actually how things work is another question.. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "None", "id": "e2akv1x", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2akv1x/", "content": "[deleted]", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "thebackpropaganda", "id": "e2azc6u", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2azc6u/", "content": "Can't agree more. I can't wait for UPN++. This is probably the kind of model that if scaled up could lead to AGI.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "PuzzledForm", "id": "e2b5on1", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2b5on1/", "content": "Isn't David Hardmaru's World Models = AGI? ", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}, {"author": "Majesticeuphoria", "id": "dz6etej", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6etej/", "content": "[This one!](https://arxiv.org/abs/1802.08195)", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "inkplay_", "id": "dz6kksi", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dz6kksi/", "content": "BEGAN I a mean just look at their results and began thinking about the possibilities...", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "Starkll7", "id": "dzwcjt3", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/dzwcjt3/", "content": "The alpha zero paper would definitely be my pick", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "kmc5500", "id": "e3row4h", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e3row4h/", "content": "I totally agree this one. No paper had bigger impact to public. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "PucheroDelAverno", "id": "e1k78a5", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e1k78a5/", "content": "I wonder what are the best scientific discoveries made by machine learning.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "None", "id": "e2xqyzs", "url": "/r/MachineLearning/comments/8kbmyn/d_if_you_had_to_show_one_paper_to_someone_to_show/e2xqyzs/", "content": "[removed]", "score": -4, "likes": 0, "upvote": -4, "downvote": 0, "comments": 0}]}, {"author": "olaf_nij", "id": "9bfr9p", "title": "[D] Having a more discussion focused MachineLearning subreddit", "url": "https://www.reddit.com/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/", "content": "After discussing with the other moderators, we are deciding to try out a new format for this subreddit. To encourage more focused discussions and a more positive community overall, we are going to emphasize the use of self-posts. Users can still post links, but only within self-posts to give the link more context for further discussion. Current exceptions to this with be arxiv.org links as we've seen in the past this already creates a fairly focused and positive discussion.\n\nWe're looking forward to seeing how these changes will continue to move the subreddit forward as it grows.", "upvote": 185, "downvote": 0, "upratio": 0.94, "subreddit": "r/MachineLearning", "comments": [{"author": "KingPickle", "id": "e5365kt", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5365kt/", "content": "I'm skeptical about this change for a few reasons:\n\n1. This sub (compared to others I'm on) really doesn't seem to suffer from a high volume of memes, shit-posts, etc. If anything, I think most of the dubious posts here *are self-posts* from newbies asking things google could answer.\n\n2. Putting all of the links inside of posts is an objectively worse UX. It requires an extra click/load process and it doesn't give you a thumbnail.\n\n3. As silly as it is, redditors like karma. And self-posts give you none. I doubt that will deter people posting links to videos/etc to academic experiments, etc. But I do wonder how much it will deter casual posters who might link a \"Two minute papers\" video (or something similar) that I'd find interesting.\n\nI'm 100% with you on the notion of fostering more discussion. I'd love that! But I'm not sold on this being the optimal path to encourage that.", "score": 56, "likes": 0, "upvote": 56, "downvote": 0, "comments": [{"author": "programmerChilli", "id": "e53fihh", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53fihh/", "content": "I think self posts do give you karma now; they changed that maybe a year ago.", "score": 21, "likes": 0, "upvote": 21, "downvote": 0, "comments": 0}, {"author": "tfburns", "id": "e53twyi", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53twyi/", "content": "1. I agree that some self-posts are from lazy newbies, but that's an argument for rule changes on self-posts, not on links (which have their own problems).\n\n2. This can be partially minimised via topic flairs and descriptive titles.\n\n3. Self-posts attract karma. But, you're right, people who like to post links are less inclined to put the effort into writing a self-post and linking within that.\n\nI'll be interested to see how this sub changes with the rule-change, but I don't think it was very necessary from my experience. I suspect we'll miss some decent blog posts and video links which could have generated interesting discussion.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "call_me_arosa", "id": "e53iajr", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53iajr/", "content": "I agree with you that it is worse UX.\nBut I don't necessarily see the point 3 as a bad thing.  \nFor example, the posts with 2 minutes paper links will now need to be inside a self post, ideally with a discussion or comment about the content.  \nFor me, one of the most disappointing aspects of r/ml as of now are links to contents with low quality and 0 significant discussions/insights about the topic in the post.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "abhishkk65", "id": "e52px0b", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52px0b/", "content": "I was curious how many posts this would affect so did a little analysis. Of the top 50 posts on the subreddit currently:\n\n* 22 are self posts\n\n* 13 are arxiv links\n\n* 3 are youtube links\n\n* 2 are twitter links\n\n* Most of the remaining 10 are blog posts.\n\nAssuming this is representative of the subreddit, this change will affect ~1/4 of future posts.\n\nMy initial reaction to this is positive, but I hope this doesn't prevent people from sharing interesting videos/academic blog posts because they feel like they have to add their commentary to it.", "score": 62, "likes": 0, "upvote": 62, "downvote": 0, "comments": [{"author": "AvatarUltima7", "id": "e52tqqp", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52tqqp/", "content": "I like it. Having some discussion and context is always super helpful before digging into the content. ", "score": 12, "likes": 0, "upvote": 12, "downvote": 0, "comments": [{"author": "tfburns", "id": "e53u0qb", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53u0qb/", "content": "Isn't the title usually descriptive enough, e.g. \"Blog post on application of RNNs on XYZ\"?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "rlrgr", "id": "e53wf5r", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53wf5r/", "content": "But then you just get a post with no comments and 5 points. There are a million blogs about RNNs so it wouldn't hurt to write 2 sentences about why this one is worthwhile, to encourage people to check it out and to open the discussion.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}]}]}, {"author": "alexmlamb", "id": "e52v3xg", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52v3xg/", "content": "It sounds like 38/50 are links, which is closer to 80% of posts.  ", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "abhishkk65", "id": "e52v7sk", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52v7sk/", "content": "I don't think you read the original post correctly. self posts and arxiv links are still allowed. This means 15/50 are links that will now be self posts in the future. That's 30% which is ~1/4 affected.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "urlwolf", "id": "e5d4ej9", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5d4ej9/", "content": "What I would like to avoid is to have this reddit turn into linkedin groups, where there's barely any discussion, just people posting links to posts.\n\n&#x200B;\n\nBut this is not what happens here. This community is very close to hacker news,which is my standard.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "jer_pint", "id": "e52zlju", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52zlju/", "content": "How hard would it be to do this retrospectively on the subs entire history? Legitimately curious at doing some actual historical analysis", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "ManyPoo", "id": "e5391vs", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5391vs/", "content": "Can we also model it with an LTSM?", "score": -3, "likes": 0, "upvote": -3, "downvote": 0, "comments": 0}]}]}, {"author": "its_ya_boi_dazed", "id": "e52tplc", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52tplc/", "content": "Personally I enjoy the drama that sometimes occurs when people get into heated discussions. Lots of good ideas get thrown around. I hope mods don\u2019t over regulate this sub until it\u2019s basically an echo chamber. ", "score": 24, "likes": 0, "upvote": 24, "downvote": 0, "comments": [{"author": "maltin", "id": "e533e3b", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e533e3b/", "content": "I agree. I was positively surprised when I launched the debate on [Deep learning and tabular data](https://www.reddit.com/r/MachineLearning/comments/9826bt/d_why_is_deep_learning_so_bad_for_tabular_data/) a while back and not only people engaged in a positive way, but the traction it gathered brought many interesting ideas to my attention that I was not aware. Overall a much better experience than the average link in the sub.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}, {"author": "Silver5005", "id": "e53ntkm", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53ntkm/", "content": "> I hope mods don\u2019t over regulate this sub until it\u2019s basically an echo chamber. \n\nSo basically reddit?", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}]}, {"author": "olBaa", "id": "e531f5t", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e531f5t/", "content": "A lot of text posts are simple questions. If you are changing the sub structure, what about restoring the questions thread that was around a year or two ago?", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}, {"author": "gdrewgr", "id": "e539l9v", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e539l9v/", "content": "This seems like the exact wrong response. I'd much rather see a link to a GOOD blog post or talk on youtube than another dumb question as a self-post.", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": 0}, {"author": "needlzor", "id": "e53erin", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53erin/", "content": "The only issue I can see is that if someone wants to post a discussion a blog post or a non-arxiv paper, they would have no way of knowing whether an existing one has already been posted. It could easily be fixed by forcing a naming convention where one has to use the title of the blog post (for example) as the title of the reddit thread, but then the moderation team would have to enforce that (or write a bot that enforces it). ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "quit_daedalus", "id": "e530668", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e530668/", "content": "Maybe creating a weekly discussion thread will help with blog posts and twitter links?", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "i-heart-turtles", "id": "e53qcs9", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53qcs9/", "content": "Is the [arxiv.org](https://arxiv.org) links rule generalized  to arbitrary paper abstracts? Or does the abstract have to be hosted on arxiv? For example, would a post linking here have to be a self-post? [https://www.ijcai.org/proceedings/2018/392](https://www.ijcai.org/proceedings/2018/392)", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "tfburns", "id": "e53ua0c", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53ua0c/", "content": "Not convinced this rule-change was needed. What's your timeline for review of this rule-change and what measures will you use to measure success/failure?", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "Prcrstntr", "id": "e54hvfe", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e54hvfe/", "content": "I thought this sub is already very impressive. It's certainly one of the more professional ones. It's nice to see lots of threads about actual problems, unlike other subs that are 90% 'how do i get started installing tensorflow'. It would be good for all those discussion threads have more than 10 comments. ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "thatguydr", "id": "e52wlyf", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52wlyf/", "content": "I love this, provided that people are smart about how they bring up blog posts within self posts. There are some *excellent* blog posts (though very rare) posted here on occasion, and putting them within the context of a self post seems very appropriate. Still, I hope that nobody decides that this is some \"everyone use clickbait titles\" free-for-all.\n\nCan we report excessive clickbait? If so, then I am 100% on board for this change. If not, then it's about 70% overall, because it's obvious what will happen.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "BeatLeJuce", "id": "e5304pp", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5304pp/", "content": "We agree. Also things like distill.pub etc. are great. We limited ourselves to arxiv for now because it's the obvious choice. Please do keep posting and discussing interesting stuff, that's the whole point!\n\nClickbait: yes! please use the modmail to send us stuff, or report it via the \"report\" button, it's super useful! We can't promise to always agree with the reports (we do get a lot of them), but we'll do our best to keep the subreddit as nice a place as possible.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}]}, {"author": "tfburns", "id": "e53tsgz", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53tsgz/", "content": "I think you should expand your link exceptions list to include relevant academic journals, since not everyone publishes on arxiv.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "Sumgi", "id": "e52r35h", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52r35h/", "content": "Have a look at r/geopolitics, they have a similar restriction.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "jer_pint", "id": "e52zcn1", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e52zcn1/", "content": "What about linking to helpful walkthroughs/tutorials/blog posts? I'm thinking about medium for example, also lots of individuals host their own blog and sometimes the articles are more than self sufficient.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "zindarod", "id": "e530sp3", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e530sp3/", "content": "This is a great idea. This sub was turning into another /r/programmng.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "LazyOptimist", "id": "e55n52m", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e55n52m/", "content": "I'm all for this experiment, let's see how this goes. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "psykocrime", "id": "e564eu0", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e564eu0/", "content": "This seems like a completely pointless change. Personally I do not support it.\n", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "rstoj", "id": "e579llt", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e579llt/", "content": "The number of highly-upvoted posts seems to have gone down since this change. \n\nHow long is the experiment, and what are the metrics you are optimising for? ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "AdditionalWay", "id": "e5a4fkg", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5a4fkg/", "content": "I love the idea. I've seen times where someone just posts and imgur link from research and it gets tons of upvotes, but a paper to the actual research itself doesn't. So 100% support this. \n\nHow does the submit a link work? If you don't have an arxiv link, the automod deleted it?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "adammathias", "id": "e5bcags", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5bcags/", "content": "Any idea why the content of this self-post was removed?\n\nr/MachineLearning/comments/9ck5wg/intuition_on_the_trick_of_reversing_input/\n\nIt happens to have been very much in the spirit of the news rules.\n", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "alpha_53g43", "id": "e5ezit8", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5ezit8/", "content": "I posted a discussion question here: https://old.reddit.com/r/MachineLearning/comments/9d2h82/what_process_is_followed_in_tagging_tumors_in/..\n\nIf we are moving towards discussion, wondering why it doesn't appear in the forums?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "CactusSmackedus", "id": "e5361bd", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5361bd/", "content": "Rules are bad don't make them", "score": -3, "likes": 0, "upvote": -3, "downvote": 0, "comments": 0}, {"author": "deepNeural", "id": "e5cqpkf", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e5cqpkf/", "content": "I have to agree with you. Most people on reddit live in their parents basement and worship trump and all things stupid. I'm afraid having posts with real merit and discussion are not gonna work so well here with these freaking morons.", "score": -1, "likes": 0, "upvote": -1, "downvote": 0, "comments": 0}, {"author": "Majesticeuphoria", "id": "e53fz22", "url": "/r/MachineLearning/comments/9bfr9p/d_having_a_more_discussion_focused/e53fz22/", "content": "How come no-one has made an AI to make their reddit posts? /s", "score": -4, "likes": 0, "upvote": -4, "downvote": 0, "comments": 0}]}, {"author": "rahul5k", "id": "9d3nnc", "title": "[D] Best Survey papers on Generative Adversarial Networks that you would suggest ?", "url": "https://www.reddit.com/r/MachineLearning/comments/9d3nnc/d_best_survey_papers_on_generative_adversarial/", "content": "", "upvote": 22, "downvote": 0, "upratio": 0.93, "subreddit": "r/MachineLearning", "comments": [{"author": "Kaixhin", "id": "e5f9741", "url": "/r/MachineLearning/comments/9d3nnc/d_best_survey_papers_on_generative_adversarial/e5f9741/", "content": "There's Ian's NIPS 2016 tutorial [written up](https://arxiv.org/abs/1701.00160), a [2017 survey](https://arxiv.org/abs/1710.07035) (disclaimer: I'm a co-author), and [another 2017 survey](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8039016). Not aware of anything more recent but also not actively researching this area.", "score": 6, "likes": 0, "upvote": 6, "downvote": 0, "comments": 0}, {"author": "Fireflite", "id": "e5fhvb7", "url": "/r/MachineLearning/comments/9d3nnc/d_best_survey_papers_on_generative_adversarial/e5fhvb7/", "content": "I rather like [The GAN Landscape: Losses, Architectures, Regularization, and Normalization](https://arxiv.org/abs/1807.04720). It's not properly a *review* paper, but it does go over many of the recent advances and compare them empirically.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "JesseOS", "id": "e5fftfb", "url": "/r/MachineLearning/comments/9d3nnc/d_best_survey_papers_on_generative_adversarial/e5fftfb/", "content": "Okay I know this doesn\u2019t really count but just looking through HyperGAN\u2019s source code, even if you don\u2019t get anything. It\u2019s interesting and you could maybe learn something.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "JesseOS", "id": "e5fftut", "url": "/r/MachineLearning/comments/9d3nnc/d_best_survey_papers_on_generative_adversarial/e5fftut/", "content": "!remind 1 day", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "phizaz", "id": "9d37rc", "title": "[D] Why don't we use running statistics for batch normalization?", "url": "https://www.reddit.com/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/", "content": "We use mini-batch statistics during train, and use population statistics during test (which using some kind of approximation like exponential averages).\n\nIn case of small mini-batch, a mini-batch statistics seems to be a poor choice.\n\nI can only wonder why we don't use a kind of exponential average more during training?", "upvote": 23, "downvote": 0, "upratio": 0.87, "subreddit": "r/MachineLearning", "comments": [{"author": "bbsome", "id": "e5f3qam", "url": "/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/e5f3qam/", "content": "[PreviousThread] (https://www.reddit.com/r/MachineLearning/comments/7jhm1t/d_why_doesnt_batch_norm_use_the_long_term_average/)  \n\n[Batch Renorm](https://arxiv.org/abs/1702.03275)", "score": 9, "likes": 0, "upvote": 9, "downvote": 0, "comments": [{"author": "phizaz", "id": "e5f3ujt", "url": "/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/e5f3ujt/", "content": "My bad. Should I delete the post?", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "bubblride", "id": "e5f8v7w", "url": "/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/e5f8v7w/", "content": "No. I am also interested in an answer. Thank you for asking.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "the_other_him", "id": "e5f7jry", "url": "/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/e5f7jry/", "content": "I wouldn\u2019t think so. You asked a valid question that someone else may have, and this question  has been addressed. ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}, {"author": "shortscience_dot_org", "id": "e5f3qey", "url": "/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/e5f3qey/", "content": "I am a bot! You linked to a paper that has a summary on ShortScience.org!\n\n**Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models** \n\n*Summary by Qure.ai*\n\n[Batch Normalization Ioffe et. al 2015](Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift) is one of the remarkable ideas in the era of deep learning that sits with the likes of Dropout and Residual Connections. Nonetheless, last few years have shown a few shortcomings of the idea, which two years later Ioffe has tried to solve through the concept that he calls Batch Renormalization.\n\n\n\nIssues with Batch Normalization\n\n\n\n- Different parameters used to compu... [[view more]](http://www.shortscience.org/paper?bibtexKey=journals/corr/1702.03275)", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "phizaz", "id": "e5fe89j", "url": "/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/e5fe89j/", "content": "In short, because it is important to make sure that we backpropagate through the \"statistics\" correctly, *which becomes hard if we use running statistics(?).* If not it is suggested in S. Ioffe and C. Szegedy, 2015 that we can have parameters blown up!", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "DaLameLama", "id": "e5fi1er", "url": "/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/e5fi1er/", "content": "PyTorch does actually use an exponential average of the mini-batch statistics, with a small default momentum of 0.1.  \nI don't know about other frameworks. Consult their docs. :)", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "phizaz", "id": "e5fio6g", "url": "/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/e5fio6g/", "content": "Really? You mean using this statistics for training? That's weird, can you provide me the source?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "iforgot120", "id": "e5fjw5p", "url": "/r/MachineLearning/comments/9d37rc/d_why_dont_we_use_running_statistics_for_batch/e5fjw5p/", "content": "Check the Github -- it's open source. But I'm pretty sure he's right because I had the same thought when reading your post.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}, {"author": "chisai_mikan", "id": "9cwq5g", "title": "[D] NIPS2018 sold out after 10-20 minutes", "url": "https://www.reddit.com/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/", "content": "Did anyone manage to register?\n\nhttps://twitter.com/seaandsailor/status/1036997049894486016\n\nI think this year they allocated a large number of tickets to authors, workshops, and good reviewers, and rightly so, since these people deserve the chance to attend and should have priority. The website says more tickets will be made available after Oct/Nov if people who are on the reserved list don't take their allocated tickets.\n\nhttps://nips.cc/Conferences/2018/WaitList", "upvote": 135, "downvote": 0, "upratio": 0.93, "subreddit": "r/MachineLearning", "comments": [{"author": "asobolev", "id": "e5dtdw5", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dtdw5/", "content": "I'm pretty sure they reserved \\~2/3 of tickets to authors and top reviewers, which is perfectly reasonable thing to do.", "score": 37, "likes": 0, "upvote": 37, "downvote": 0, "comments": [{"author": "mtocrat", "id": "e5end8g", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5end8g/", "content": "There's nothing reasonable about this. I'm frankly surprised by how accepting people here are that this happened. If the top conference in a field is closed to researchers because there's too much hype then that's a problem. There are multiple ways that could have been handled better. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "tobyclh", "id": "e5ex98h", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ex98h/", "content": "This is not about exclusiveness. This is about letting people who make this event possible to participate.\n\nAnd no one stops anybody from submitting a well written paper and get your ticket reserved either.", "score": 12, "likes": 0, "upvote": 12, "downvote": 0, "comments": [{"author": "mtocrat", "id": "e5f9dlp", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f9dlp/", "content": "What's unreasonable is that this was expected to happen and these were the only precautions they took. Requiring a nips paper every year to attend is not an acceptable solution. ", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "asobolev", "id": "e5fc0ki", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fc0ki/", "content": "Good reviewers also got a ticket reserved, and some are reserved for workshops, which should be easier to get into than the regular conference track.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "ThisIsMyStonerAcount", "id": "e5f6n5o", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f6n5o/", "content": "A conference is a place were researchers meet and exchange ideas. IMO it makes all the sense in the world to make sure that they get the spots, because *that's what it's all about in the first place*. But I'm curious: how do you think it could have been handled better?", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "mtocrat", "id": "e5f9gob", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f9gob/", "content": "For one, there are bigger conferences in adjacent fields. The obvious solution would have been to accommodate more people. If you don't want to be a giant conference, then a longer term solution would be to split it up. ", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "asobolev", "id": "e5fc2fz", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fc2fz/", "content": "You might as well consider this a unilateral split then.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "mtocrat", "id": "e5fccij", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fccij/", "content": "I'm sure that's exactly what's going to happen, people will spread out and attend other conferences. But it's unorganized and not along topic lines which makes this a terrible situation for researchers who would like to meet other people in their fields", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "asobolev", "id": "e5fdwhw", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fdwhw/", "content": "Why should somebody be organizing this? Who should it be? Let the community organize itself.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "mtocrat", "id": "e5fdyea", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fdyea/", "content": "Who do you think organizes NIPS? This is the community. If NIPS wants to be smaller it should restrict it's scope", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}, {"author": "Brudaks", "id": "e5fed4w", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fed4w/", "content": "An unilateral split would be if NIPS chose to reduce/restrict its scope in some aspects, thus forcing these papers and those sub-communities to another conference.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}, {"author": "anditcametoass", "id": "e5f1jfg", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f1jfg/", "content": "Who the fuck is going to present then, if they don\u2019t save tickets for the authors?", "score": 11, "likes": 0, "upvote": 11, "downvote": 0, "comments": [{"author": "mtocrat", "id": "e5f9jtf", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f9jtf/", "content": "It's unacceptable because it's the absolute bare minimum thing they could have done, not because they shouldn't have done it.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "Jonno_FTW", "id": "e5f5dwf", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f5dwf/", "content": "Slideshows will just be run on a loop if the author didn't get a ticket in time.\n\nEdit: /s", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "ThisIsMyStonerAcount", "id": "e5f6o33", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f6o33/", "content": "Why would people even submit a paper to a conference if they cannot even present there? They might as well turn NIPS into a journal, then.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "Jonno_FTW", "id": "e5f9sj7", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f9sj7/", "content": "Yeah it seems ludicrous for anyone to think a paper author shouldn't at least be offered the opportunity to present. I was making a joke above though.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}, {"author": "Mefaso", "id": "e5escz6", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5escz6/", "content": "It certainly makes more sense than cutting the tickets for researchers", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "baylearn", "id": "e5dq0t8", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dq0t8/", "content": "They prob did this to encourage better review writing ...", "score": 28, "likes": 0, "upvote": 28, "downvote": 0, "comments": 0}, {"author": "juancamilog", "id": "e5drrj6", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5drrj6/", "content": "Hope the lucky people are aware about the weather in Montreal.", "score": 18, "likes": 0, "upvote": 18, "downvote": 0, "comments": [{"author": "Bewaretheicespiders", "id": "e5duigy", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5duigy/", "content": "December's not too cold (compared to february), but heh, at least people will come for the right reasons.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}, {"author": "IborkedyourGPU", "id": "e5fdxh9", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fdxh9/", "content": "It sure as hell beats the weather in Toronto.", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": 0}]}, {"author": "svaisakh", "id": "e5dss7s", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dss7s/", "content": "Reminds me of ICOs during the ethereum craze.", "score": 42, "likes": 0, "upvote": 42, "downvote": 0, "comments": [{"author": "PK_thundr", "id": "e5em31z", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5em31z/", "content": "Does this make anyone else worried about a bubble?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "blowjobtransistor", "id": "e5ep0ui", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ep0ui/", "content": "Nope. If you build a product that solves a real problem that costs people a lot of money, they will pay you for it.", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": [{"author": "0_-", "id": "e5erdt0", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5erdt0/", "content": "The issue is that people would rather play video games and watch YouTube videos, which the 1080 ti you have in order to practice machine learning, is very capable of handling. It is very hard to learn math and make very good products. \n\nYou look at all these coins and a lot of them were terrible coins. There were a few actual good coins that had people working on them for a long time, who actually came from the industry, even then outside factors ended up obscuring them.\n\nPeople understand the power of machine learning, very good, but finding hard working and capable men is another matter. People have their golden eras of programming, people can get burnt out on programming, and these hype environments are the perfect environment to get burnt out in.\n\nYou can give money to people who solve a real problem, you can also give money to people who claim to solve a problem, and who are very good at putting on the appearance that they can solve a problem. If you're just a journalist or riding the hype train, especially in this sort of garbage media environment that exists nowadays, a lot of people are gonna get suckered and lose out. And they deserve to lose their money. This stuff is really hard and you should assume that capable people are very rare and hard to find. They will also want to tend to work on their own small projects.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}]}, {"author": "stochastic_zeitgeist", "id": "e5dw36k", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dw36k/", "content": "It got sold out when I was filling in CC details :\\. It'd be great if we have a lockin of five minutes after you checkout for you to fill the CC details (something like Eventbrite has in place).", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": [{"author": "Itsalongwaydown", "id": "e5dzocr", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dzocr/", "content": "or have auto form fill out saved in your browser", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}]}, {"author": "thebackpropaganda", "id": "e5dpwq9", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dpwq9/", "content": "Damn I'm sure happy I decided to write good reviews this NIPS and saved myself a spot. I would have never gotten the tickets without it.", "score": 22, "likes": 0, "upvote": 22, "downvote": 0, "comments": [{"author": "mechatron91", "id": "e5e894d", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e894d/", "content": "I am genuinely curious. How do you know that your reviews are good ? \n\nI recently discovered that reviews are posted on papers.nips.cc and I am trying to learn how to evaluate and compare & contrast papers based on that. A good metric to evaluate a review would be really helpful to me. Thanks in advance. ", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": [{"author": "Nosferax", "id": "e5embd7", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5embd7/", "content": "Scores were assigned to reviewers by chairs, so in a way peer review of peer review.", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": [{"author": "PokerPirate", "id": "e5ew3y3", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ew3y3/", "content": "Those scores aren't made public though, are they?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "thebackpropaganda", "id": "e5epb1t", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5epb1t/", "content": "I don't think I'm the best example here. I invite people who were in the top 218 to answer this question.\n\nIn my opinion, there is a difference between reading a paper and reading a paper to review it, and it's important to not read an under-review paper the same way you'd read a peer-reviewed paper or a paper from trusted authors. To read a paper to review it, one has to read it *critically* , i.e. identify the weakest links in the paper's narrative which break the hypothesis. This is hard because most people have spent a lot of time reading high-quality books and papers which they have no reason to doubt. Unless you took a philosophy class (and didn't sleep through it), it's rare to have practice reading a piece critically, which is necessary to write good reviews. The core idea is to ask yourself if the paper is actually providing evidence for the claims being made. This is something that obviously comes with practice, but it could be useful to keep in mind when next you're reviewing or just reading a paper from arxiv.\n\nAnother thing I incorporate in my reviews is suggestions for improvements. Reviewing is not an adversarial game (for the most part). 5 out of 6 of my reviews contained explicit recommendations for how to give better evidence for whatever hypothesis they were presenting. Note that this doesn't just mean \"get better SOTA before publishing\" but more fine-grained experiments on perhaps even toy tasks on how they can corroborate their hypothesis better.\n\nIt obviously also helps to be a well-read researcher. Do you know the nitty-gritty details that outsiders won't? Do you know what metrics and evaluation protocol are currently used for a certain problem area? Do you know if a certain dataset has fallen out of favor due to certain problems identified in it? Adding all this information in the review if relevant is something only someone immersed in the area can do, and the area chair would be able to appreciate that. Such things are more informative than just asking for more ablations which is something anyone can ask and doesn't leverage your expertise in the area.\n\nI'm guessing this is already well-known but do a good job of summarizing the paper, and don't just copy from the paper itself. Explain the paper to someone who is already an expert in this area (like the area chair) in a few lines. Pick out the important bits of the paper that the paper did not emphasize but is worth emphasizing such as things like \"the proposed method X modifies existing method Y by doing Z\". This is something the paper might not emphasize but is obviously worth emphasizing not only to judge novelty, but also to understand the idea better.\n\nAgain, I can't really claim to be really good at reviewing, and I do wish I can learn from one of those top 218 reviewers myself.\n\n", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": [{"author": "approximately_wrong", "id": "e5et53w", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5et53w/", "content": "I was lucky enough to be in the top 218. The AC's assigned me papers that were well within my areas of expertise and I did my best to approach the reviews with the mindset you described above. Personally, I also find it important to identify whether or not the hypotheses put forth in the paper are actually consistent with and well-justified by the experimental and mathematical claims (just because the numbers are great doesn't mean the authors' explanation of *why* their algorithm improved things is correct). I think it also helped that I was involved in a particularly lively discussion where the reviewers collectively discovered a particular limitation of the paper that heavily influenced the AC's final decision; it was quite enlightening for all of us! :)", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": 0}]}]}]}, {"author": "IborkedyourGPU", "id": "e5dq2m4", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dq2m4/", "content": "<9 minutes to be precise. Shit.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "geodesic42", "id": "e5ew1zq", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ew1zq/", "content": "Heard it was 11 mins at 38 sec", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "IborkedyourGPU", "id": "e5fdro5", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fdro5/", "content": "Yeah, that was the number published by NIPS but I'm pretty sure I couldn't find a ticket @ 8:09 AM...maybe some lag issue? Or maybe tutorials and admissions didn't go sold out at the same time.  Anyway, the issue still stands. I mean, this is not a Taylor Swift gig, it's a scientific conference.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}, {"author": "Aargau", "id": "e5dude3", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dude3/", "content": "I set my alarm for midnight, but missed registration as it opened at 8AM PDT.", "score": 7, "likes": 0, "upvote": 7, "downvote": 0, "comments": 0}, {"author": "abhishkk65", "id": "e5dtxl5", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dtxl5/", "content": "People are freaking out without any context.\n\nAbout ~2000 tickets were sold today. 6000 more tickets are reserved for conference and workshop authors.\n\nEveryone who needs to go to NIPS will be able to. Yes, they sold out fast, but NIPS has always been more popular than the other ML conferences anyway. ICLR is an order of magnitude smaller.", "score": 18, "likes": 0, "upvote": 18, "downvote": 0, "comments": [{"author": "Aargau", "id": "e5duo7i", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5duo7i/", "content": "\\> Everyone who needs to go to NIPS will be able to.  \n\n\nGiven Andrej Karpathy wasn't able to go...  \n\n\n[https://twitter.com/karpathy/status/1037011082777251841](https://twitter.com/karpathy/status/1037011082777251841)", "score": 11, "likes": 0, "upvote": 11, "downvote": 0, "comments": [{"author": "abhishkk65", "id": "e5dw5zc", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dw5zc/", "content": "Needs is different from wants. There are people on Twitter still complaining about why registration is open before paper acceptances even though the PCs have repeatedly said authors of accepted papers have reserved tickets and register later.", "score": 11, "likes": 0, "upvote": 11, "downvote": 0, "comments": 0}, {"author": "petrified_piranha", "id": "e5fdfd0", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fdfd0/", "content": "Why should Karpathy be at an advantage assuming he didn't submit or review ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}, {"author": "ThisIsMyStonerAcount", "id": "e5f6pzc", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f6pzc/", "content": ">  ICLR is an order of magnitude smaller.\n\nnot really, last ICLR was 3k people or so. So it's about half the size.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "tpinetz", "id": "e5ffrdj", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ffrdj/", "content": "which is an order of magnitude smaller ;)", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "crazycrazycrazycrazy", "id": "e5fjdyg", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fjdyg/", "content": "10 != 2", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}, {"author": "INDEX45", "id": "e5e0r5m", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e0r5m/", "content": "This is dumb. I\u2019m glad that the reviewers and authors can attend, but this leaves out hundreds (thousands?) in academia and thousands in industry who are regular or legitimate attendees simply because they didn\u2019t have time to submit or review this year. \n\nAnd furthermore, is it really that great of an idea to have the vast majority of people attending be those that were part of that process? It seems more like a congratulations party for those who were accepted rather than an opportunity for a diverse and vigorous debate/discussion on the rare occasion where all these same types of people just happen to be in the same spot. It\u2019s also horrible for the authors, particularly of the less popular papers, because it reduces the chances they have to discuss and \u201cpromote\u201d their work.\n\nIf it\u2019s a one time SNAFU, that\u2019s fine, it happens. But the conference committees and chairs need to do better if the conference isn\u2019t going to turn into a parody of itself (hint: charge industry *way* more, $750 is stupidly cheap for them, and use it to underwrite the growth, increase student subsidies, etc., and ask for *significantly* higher donations.)\n\nPS I should add, what about those who were trying to register but it took forever because of shitty connections? You\u2019re telling me researchers in Africa, for example, are SOL just because of where they live? This needs to be done better. ", "score": 14, "likes": 0, "upvote": 14, "downvote": 0, "comments": 0}, {"author": "goko19", "id": "e5dwo4j", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dwo4j/", "content": "11 minutes 38 seconds to be precise. They posted on fb.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "ManageableGrip", "id": "e5esccr", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5esccr/", "content": "Precise or accurate?", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "goko19", "id": "e5ezrqy", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ezrqy/", "content": "Both.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}]}]}, {"author": "mlawaythrow", "id": "e5dprwe", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dprwe/", "content": "[hot-take]\nIt would almost seem obvious that academic conferences should limit the number of participants who do not have a university affiliation. Of course what do I know, I'm just a graduate student who would like to attend the landmark conference in my field. If you've had a paper accepted, I hope you are looking forward to presenting your poster to recruiters and VC's asking you what's your GAN algorithm's go to market plan.\n[/hot-take]", "score": 55, "likes": 0, "upvote": 55, "downvote": 0, "comments": [{"author": "_untom_", "id": "e5dr847", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dr847/", "content": "To be fair, especially in ML there is a lot of research happening in industry as well, and some of their research is very innovative, so you cannot limit yourself to academia (even some of this year's organizers are industry folk). Additionally, people with accepted posters as well as reviewers (so arguably the people most active in current research) did get reserved spots, so there will be enough researchers at the conference. Also, judging from the last few conferences, the problem weren't so much VCs/recruiters, but all the newcomers and people who want to get into to the field (I doubt the former would crowd in front of posters during presentations so much).\n\nIn any case, I'm sure the commitee will figure out something to make sure more research-y folk can attend, and somehow try to limit VC/recruiters from flooding the conference.", "score": 26, "likes": 0, "upvote": 26, "downvote": 0, "comments": 0}, {"author": "a_draganov", "id": "e5dqz6a", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dqz6a/", "content": "I understand where you're coming from, but idk that I entirely agree. The field's current state is equally spread across academia and industry. Denying one of those halves will just exclude a significant amount of researchers according to an arbitrary premise.", "score": 50, "likes": 0, "upvote": 50, "downvote": 0, "comments": [{"author": "mlawaythrow", "id": "e5drbur", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5drbur/", "content": "Sure, my phrasing might be wrong. But you must agree that there's a difference between [google brain, etc] researchers and what we would generally call 'industry'. Maybe the delineation would be done differently, and I don't know the specific rules that would be inclusive to all researchers, however I do know a lot of people who have published here before and will not be able to attend. Maybe that is worth addressing.", "score": 29, "likes": 0, "upvote": 29, "downvote": 0, "comments": [{"author": "a_draganov", "id": "e5drv78", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5drv78/", "content": "Definitely worth addressing, I agree. I doubt there's a system that will make everyone happy, but a lot of them would be better than the current first-come-first-serve.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}, {"author": "Nimithryn", "id": "e5dva2y", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dva2y/", "content": "\" you must agree that there's a difference between \\[google brain, etc\\] researchers and what we would generally call 'industry' \"\n\nCare to elaborate? I'm not sure I do agree. ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "mlawaythrow", "id": "e5dvnqs", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dvnqs/", "content": "Happy to. By industry I am referring to anyone who works in a tech company or invests in one, i.e. product manager, recruiter, investor, etc. By [* researchers] I am referring to people who are doing machine learning research in a company lab. \n\nMy point is that *researchers* (independent, academic, in an industry lab) should be the main participants of a machine learning research conference.\n\nedit: btw you can quote by using '> text you are quoting'.", "score": 27, "likes": 0, "upvote": 27, "downvote": 0, "comments": [{"author": "darosati", "id": "e5er7x2", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5er7x2/", "content": "as \"researcher\" aka a person in an industry lab - i never thought of this distinction. Though i am doing ML research as part of my work I consider myself Industry and before you clarified i was worried you were suggesting people like me be exluded. I also think I agree with you but I think most people in my position would consider themselves Industry and may even be product managers and ceos as well as trained researchers making product as a form of investigation.... ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "mlawaythrow", "id": "e5fcy8b", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fcy8b/", "content": "> making product as a form of investigation....\n\nBut that's not the type of research that is presented at NIPS. You can't submit a paper on how your new dashboard is using machine learning to get more users to click on something. I know product managers are analytical in nature and deal with numbers a lot, however the distinction is clear.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}, {"author": "Nimithryn", "id": "e5dx9z4", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dx9z4/", "content": "Thanks, that makes sense, I think I agree with that.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}, {"author": "mcorah", "id": "e5dykku", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dykku/", "content": "I'm not familiar with how NIPS is doing things right now so I am just spitballing, but industry researchers who are publishing in the conference will definitely have tickets, and I'd bet industry sponsorships come with a handy number of tickets too (true for another conference I was involved with).", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "GuardsmanBob", "id": "e5dscug", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dscug/", "content": "I'd mark this under growing pains and while they can suck, they are ultimately good problems to have.\n\nI think instead of academia trying to 'reclaim nips' it seems more obvious that there maybe at some point it really should be two conferences, one for 'hot ML/industry stuff' and another for 'hopeful academic endeavors' that do not have any applications yet.\n\n\nIn particular I think this would help fledgling research that isn't eye catching or produces SOTA results, but might provide a path forward if given more attention.\n\nThe way this could be done in practice is to hold a 'boring nips' a few days before/after the 'public nips' and be very strict about not having SOTA result papers and other eye catchy stuff in the 'boring section'.\n\nAnyway, just my thoughts.", "score": 10, "likes": 0, "upvote": 10, "downvote": 0, "comments": [{"author": "asobolev", "id": "e5dvcpk", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dvcpk/", "content": "No need for the split, just prioritize people that actually contribute to science. And this seems to be the case this year, at least to some degree.", "score": 8, "likes": 0, "upvote": 8, "downvote": 0, "comments": [{"author": "PaleVacation", "id": "e5ewfsb", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ewfsb/", "content": "Wow, I cant believe this is an argument people are having. I feel like everyone here is thinking they are more valuable to the field than the other 50k (?) people that cant go but wanted to. Even more than that, they view the conference as if it was their right. \n\n&#x200B;\n\nPeople being more recognized in an industry doesn't mean they deserve everything that industry has to offer at the expense of others with enough passion to time things well... If that were the case, what would people looking to grow get? What is the future of the industry if we push people with passion aside for those with experience?  \n\n\nPeople that got a ticket got one because they love the field and put forth the effort. They want to learn just as much as the rest of us.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": [{"author": "asobolev", "id": "e5fbx4t", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fbx4t/", "content": ">I feel like everyone here is thinking they are more valuable to the field than the other 50k (?) people that cant go but wanted to\n\nThere's no capacity to fit everyone, so you have to chose. And prioritized choice is better than random.\n\n&#x200B;\n\nI didn't get your point regarding the industry.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": [{"author": "PaleVacation", "id": "e5fc95l", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fc95l/", "content": "Selecting based on who is hitting refresh while waiting for tickets is a pretty good way to identify passion. I wouldn't say it is random.", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": [{"author": "asobolev", "id": "e5fdnza", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fdnza/", "content": "Only 25-33% were selected this way. And in terms of actual contribution to the science (remember, we're talking about *scientific conference*) this is not that different from random.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": [{"author": "PaleVacation", "id": "e5fj3u8", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fj3u8/", "content": "25-33% may be given directly due to their contribution but the rest are given to people based on passion. The people that didnt get tickets were not as prepared as those that did. I wouldn't say it is random. ", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}]}]}]}, {"author": "INDEX45", "id": "e5e2zi1", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e2zi1/", "content": "The problem is this happened last year too. It\u2019s not just growing pains, it\u2019s poor management. ", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "INDEX45", "id": "e5e5o95", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e5o95/", "content": "Don\u2019t you think it\u2019s easier to just get a bigger place before we start excluding people based on their *affiliation*? This isn\u2019t a secret club, it\u2019s science. Or at least it\u2019s supposed to be. \n\nLike... this is not a new problem, there are thousands and thousands of bigger conferences every year that handle size just fine. There\u2019s absolutely no reason why we cant accept everyone who wants to attend, absolutely none. Besides, regarding your specific point, lots of academia were excluded this time. Lots of grad students like yourself. Lots of researchers overseas.\n\nRight now the vast majority of people going are either those who had their papers accepted, and those who approved their papers. Which is downright weird, and frankly, very unhealthy. ", "score": 11, "likes": 0, "upvote": 11, "downvote": 0, "comments": 0}, {"author": "SedditorX", "id": "e5eutzj", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5eutzj/", "content": "I'm surprised this is getting any votes. Doesn't the insinuation that a meaningful proportionof those who managed to get tickets were \"recruiters and VCs\" - or otherwise \"undeserving\" - merit even the *slightest* evidence?\n\nThe claim seems so stupid at face value that I'm wondering if I'm missing something.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "mlawaythrow", "id": "e5fcv2j", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fcv2j/", "content": "This is not a 'claim'. If the 2000 tickets that were sold are all researchers then my post has no point. Since my colleagues and I were not able to get tickets, together with the other dozens of researchers that are complaining on twitter, the post is simply a rant about the conference being unaccessible to those who participate in the science part of the field.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "SedditorX", "id": "e5fd90x", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fd90x/", "content": "This isn't what you said originally. You specifically claimed that a significant portion of the people who actually did get tickets are undeserving. I think you would agree that there is no evidence that certain types of people got a disproportionate amount of the tickets so it's unfair to malign members of industry. \n\nThat's all I was calling out. Otherwise I don't disagree with your second point about the conference being inaccessible.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}]}, {"author": "olBaa", "id": "e5drggu", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5drggu/", "content": "Don't worry, the general crowd will be around cool and hyped posters/sessions. Nothing to worry about", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "AlexiaJM", "id": "e5du3tl", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5du3tl/", "content": "This is an horrible idea which screams elitism. This would remove all researchers from the industry, government, independent researchers, and journalists.", "score": 18, "likes": 0, "upvote": 18, "downvote": 0, "comments": [{"author": "mlawaythrow", "id": "e5dudy5", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dudy5/", "content": "https://www.reddit.com/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5drbur/", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "mlawaythrow", "id": "e5dw8y9", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dw8y9/", "content": "Sorry you are getting down-voted, your point is valid. My rant was not meant to differentiate between independent/industry/academic researchers; all of these people participate in the progress of the field and make valid scientific contributions, the conference should be accessible to them more-so than the 10 new SV startups that bought tickets in bulk and will write medium posts about how cool their recruitment booth was. ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}]}]}, {"author": "gdrewgr", "id": "e5enfui", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5enfui/", "content": "in most fields, nobody who's not presenting even WANTS to register for a conference.", "score": 5, "likes": 0, "upvote": 5, "downvote": 0, "comments": 0}, {"author": "tihokan", "id": "e5dqcf5", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dqcf5/", "content": "Hopefully next year they'll adopt something similar to what was done at the Montreal AI Symposium this year, with a random draw instead of the \"first come first served\" rule.", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": [{"author": "INDEX45", "id": "e5e0yse", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e0yse/", "content": "It would be better if they just found a bigger venue that can accommodate everyone. ", "score": 13, "likes": 0, "upvote": 13, "downvote": 0, "comments": [{"author": "anyonethinkingabout", "id": "e5f9rhj", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f9rhj/", "content": "They'll be going to stadiums soon then", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}]}, {"author": "AlexiaJM", "id": "e5du728", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5du728/", "content": "Let's hope that they at least do this for the waitlist. They should have planned this already, it's crazy...", "score": 4, "likes": 0, "upvote": 4, "downvote": 0, "comments": 0}]}, {"author": "yanziang", "id": "e5e0zft", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e0zft/", "content": "Decision for accept/reject status is now available in your cmt site.", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "mmxgn", "id": "e5ehuck", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ehuck/", "content": "Holy that's crazy. What's the set list.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "Anthemiusser", "id": "e5f8vwg", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f8vwg/", "content": "Reviewer here, with a reserved ticket (apparently I was in the top 30% of reviewers, if that means anything); given that I spent more than a week on my five papers, fastidiously going over them, I think it only fair that some of us have tickets reserved. That was a lot of hard work, and it is nice to be recognised for one's reviewing for once.", "score": 2, "likes": 0, "upvote": 2, "downvote": 0, "comments": 0}, {"author": "massive_muqran", "id": "e5e3d72", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e3d72/", "content": "I got an email saying I received a free registration for being in the highest scoring reviewers, but I cannot attend due to family constraints.  Does anyone know if I can transfer it?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "julianCP", "id": "e5ea5nr", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ea5nr/", "content": "I wonder what is recommended NIPS or AAAI ?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "Anthemiusser", "id": "e5f90gg", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f90gg/", "content": "Depends what you're after really, they are quite different in scope.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "julianCP", "id": "e5f9dl5", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f9dl5/", "content": "So, in what respect are they different?", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "Anthemiusser", "id": "e5fcgrk", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5fcgrk/", "content": "Standard ar nips is much higher, more theory and of course they have the neuroscience track. AAAI is more of a jack of all trades and as another commenter said, has gone down in quality over the years. Why I do not know.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}]}]}, {"author": "kau_mad", "id": "e5f9re7", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f9re7/", "content": "NIPS of course. Quality of accepted papers at AAAI has gone down so fast. Most of the AAAI papers are a waste of time.", "score": 0, "likes": 0, "upvote": 0, "downvote": 0, "comments": 0}]}, {"author": "rstoj", "id": "e5efh5u", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5efh5u/", "content": "This is total madness!", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "synaesthesisx", "id": "e5ehobv", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5ehobv/", "content": "Expect to see these at a huge premium on Stubhub", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": 0}, {"author": "PM_ME_NIPS_TUTORIALS", "id": "e5f9g64", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5f9g64/", "content": "Selling my NIPS registration USD $3,000.\n\nIncludes main conference, workshop, and tutorials.", "score": -1, "likes": 0, "upvote": -1, "downvote": 0, "comments": 0}, {"author": "singularineet", "id": "e5dtons", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dtons/", "content": "Next year: [Vickrey auction](https://en.wikipedia.org/wiki/Vickrey_auction)!\n\n(This means everyone who wants a ticket puts in a bid of the maximum they're willing to pay, then if there are N registrations available the price that each person pays is the same, namely the Nth-highest bid. So of all the people who get a ticket, all but one person pay less than they bid. It encourages honest bids, and has some nice fairness properties.)", "score": -7, "likes": 0, "upvote": -7, "downvote": 0, "comments": [{"author": "abhishkk65", "id": "e5du38q", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5du38q/", "content": "1. The goal of this conference isn't to make money\n\n2. You want to make sure poor grad students can't go to a conference because this is how you do it", "score": 18, "likes": 0, "upvote": 18, "downvote": 0, "comments": [{"author": "INDEX45", "id": "e5e12p2", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e12p2/", "content": "Then charge industry more and students less. It\u2019s not rocket science. ", "score": 3, "likes": 0, "upvote": 3, "downvote": 0, "comments": 0}, {"author": "singularineet", "id": "e5e0q3c", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e0q3c/", "content": "[Computational Humor](https://en.wikipedia.org/wiki/Computational_humor) much?\n\nAlso, I would note that a Vickrey auction does not maximize revenue. But to the extent that this raises more money, the funds could be used on scholarships for students and poor starving academics. Uber could also contribute by allowing destitute academics to earmark their earnings as drivers directly to NIPS registration fees, pre-taxes.", "score": 1, "likes": 0, "upvote": 1, "downvote": 0, "comments": [{"author": "WikiTextBot", "id": "e5e0qdp", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5e0qdp/", "content": "**Computational humor**\n\nComputational humor is a branch of computational linguistics and artificial intelligence which uses computers in humor research. It is a relatively new area, with the first dedicated conference organized in 1996.The first \"computer model of a sense of humor\" was suggested by\n\nSuslov as early as 1992. Investigation of the general scheme of information processing shows the possibility of a specific malfunction, conditioned by the necessity of a quick deletion from consciousness of a false version. This specific malfunction can be identified with a humorous effect on psychological grounds: it exactly corresponds to incongruity-resolution theory.\n\n***\n\n^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&message=Excludeme&subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/MachineLearning/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot)   ^]\n^Downvote ^to ^remove ^| ^v0.28", "score": -1, "likes": 0, "upvote": -1, "downvote": 0, "comments": 0}]}, {"author": "JAGGI_JATT", "id": "e5dzvy4", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dzvy4/", "content": "They can take a loan to go there :)", "score": -1, "likes": 0, "upvote": -1, "downvote": 0, "comments": 0}]}]}, {"author": "evc123", "id": "e5dsuov", "url": "/r/MachineLearning/comments/9cwq5g/d_nips2018_sold_out_after_1020_minutes/e5dsuov/", "content": "If you have a spare ticket (preferably workshop), PM me and I'll buy it.", "score": -6, "likes": 0, "upvote": -6, "downvote": 0, "comments": 0}]}]}